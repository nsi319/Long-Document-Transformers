2021-02-17 07:04:23.179671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
02/17/2021 07:04:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
02/17/2021 07:04:24 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Feb17_07-04-24_fa638e4755cc', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], sortish_sampler=False, predict_with_generate=True)
02/17/2021 07:04:24 - WARNING - datasets.builder -   Using custom data configuration default-c67879d98cbd82da
02/17/2021 07:04:24 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c67879d98cbd82da/0.0.0/965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93)
loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "transformers_version": "4.3.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "transformers_version": "4.3.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

loading file https://huggingface.co/allenai/led-base-16384/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/4fb25bb1f9a942a2e2930029211b4a7deaeb18b62f6e5ce6d59730c90da51373.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
loading file https://huggingface.co/allenai/led-base-16384/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/087e8f4306cbf22e21907929074344a3b0a46bd680a118eb6267cd5a2bcec5b2.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
loading file https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer.json from cache at None
loading weights file https://huggingface.co/allenai/led-base-16384/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c8f7e4603efbc329ce921b34057d78880dead50f45b2a1648b3a06ca6eb17f51.201222b06d46289037a8dccc57548abc8eb81ba042d3762214ac15c9691ff8c7
All model checkpoint weights were used when initializing LEDForConditionalGeneration.

All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.
100% 1/1 [00:01<00:00,  1.01s/ba]
100% 1/1 [00:00<00:00,  6.18ba/s]
The following columns in the training set don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: .
The following columns in the evaluation set don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: .
***** Running training *****
  Num examples = 86
  Num Epochs = 3
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 1
  Gradient Accumulation steps = 1
  Total optimization steps = 258
  1% 3/258 [00:05<07:55,  1.86s/it]Input ids are automatically padded from 4785 to 5120 to be a multiple of `config.attention_window`: 1024
  2% 4/258 [00:07<07:49,  1.85s/it]Input ids are automatically padded from 3638 to 4096 to be a multiple of `config.attention_window`: 1024
  2% 5/258 [00:08<07:16,  1.73s/it]Input ids are automatically padded from 3360 to 4096 to be a multiple of `config.attention_window`: 1024
  4% 10/258 [00:17<07:16,  1.76s/it]Input ids are automatically padded from 3529 to 4096 to be a multiple of `config.attention_window`: 1024
  6% 15/258 [00:26<07:12,  1.78s/it]Input ids are automatically padded from 3868 to 4096 to be a multiple of `config.attention_window`: 1024
  6% 16/258 [00:27<06:44,  1.67s/it]Input ids are automatically padded from 4709 to 5120 to be a multiple of `config.attention_window`: 1024
  7% 17/258 [00:29<06:54,  1.72s/it]Input ids are automatically padded from 2641 to 3072 to be a multiple of `config.attention_window`: 1024
  7% 18/258 [00:30<06:08,  1.53s/it]Input ids are automatically padded from 2057 to 3072 to be a multiple of `config.attention_window`: 1024
  8% 20/258 [00:33<06:06,  1.54s/it]Input ids are automatically padded from 4747 to 5120 to be a multiple of `config.attention_window`: 1024
 10% 26/258 [00:44<06:54,  1.78s/it]Input ids are automatically padded from 3702 to 4096 to be a multiple of `config.attention_window`: 1024
 11% 28/258 [00:47<06:44,  1.76s/it]Input ids are automatically padded from 4810 to 5120 to be a multiple of `config.attention_window`: 1024
 12% 31/258 [00:53<06:54,  1.82s/it]Input ids are automatically padded from 3123 to 4096 to be a multiple of `config.attention_window`: 1024
 16% 42/258 [01:13<06:39,  1.85s/it]Input ids are automatically padded from 3486 to 4096 to be a multiple of `config.attention_window`: 1024
 17% 45/258 [01:18<06:26,  1.81s/it]Input ids are automatically padded from 3519 to 4096 to be a multiple of `config.attention_window`: 1024
 18% 46/258 [01:19<06:01,  1.71s/it]Input ids are automatically padded from 4063 to 4096 to be a multiple of `config.attention_window`: 1024
 19% 49/258 [01:25<06:03,  1.74s/it]Input ids are automatically padded from 3767 to 4096 to be a multiple of `config.attention_window`: 1024
 21% 53/258 [01:32<06:14,  1.82s/it]Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024
 21% 55/258 [01:35<05:45,  1.70s/it]Input ids are automatically padded from 4232 to 5120 to be a multiple of `config.attention_window`: 1024
 23% 60/258 [01:44<06:07,  1.85s/it]Input ids are automatically padded from 4419 to 5120 to be a multiple of `config.attention_window`: 1024
 24% 62/258 [01:48<06:10,  1.89s/it]Input ids are automatically padded from 3763 to 4096 to be a multiple of `config.attention_window`: 1024
 24% 63/258 [01:50<05:47,  1.78s/it]Input ids are automatically padded from 1747 to 2048 to be a multiple of `config.attention_window`: 1024
 25% 65/258 [01:52<05:11,  1.61s/it]Input ids are automatically padded from 2980 to 3072 to be a multiple of `config.attention_window`: 1024
 26% 68/258 [01:57<05:27,  1.73s/it]Input ids are automatically padded from 3472 to 4096 to be a multiple of `config.attention_window`: 1024
 27% 69/258 [01:59<05:13,  1.66s/it]Input ids are automatically padded from 4958 to 5120 to be a multiple of `config.attention_window`: 1024
 27% 70/258 [02:01<05:25,  1.73s/it]Input ids are automatically padded from 511 to 1024 to be a multiple of `config.attention_window`: 1024
 29% 75/258 [02:09<05:29,  1.80s/it]Input ids are automatically padded from 3500 to 4096 to be a multiple of `config.attention_window`: 1024
 30% 77/258 [02:12<05:24,  1.79s/it]Input ids are automatically padded from 3150 to 4096 to be a multiple of `config.attention_window`: 1024
 30% 78/258 [02:14<05:05,  1.70s/it]Input ids are automatically padded from 4643 to 5120 to be a multiple of `config.attention_window`: 1024
 31% 80/258 [02:18<05:23,  1.82s/it]Input ids are automatically padded from 4841 to 5120 to be a multiple of `config.attention_window`: 1024
 32% 82/258 [02:22<05:30,  1.88s/it]Input ids are automatically padded from 3744 to 4096 to be a multiple of `config.attention_window`: 1024
 33% 85/258 [02:27<05:23,  1.87s/it]Input ids are automatically padded from 4173 to 5120 to be a multiple of `config.attention_window`: 1024
 35% 90/258 [02:37<05:16,  1.88s/it]Input ids are automatically padded from 4173 to 5120 to be a multiple of `config.attention_window`: 1024
 35% 91/258 [02:38<05:14,  1.88s/it]Input ids are automatically padded from 3763 to 4096 to be a multiple of `config.attention_window`: 1024
 36% 94/258 [02:44<05:03,  1.85s/it]Input ids are automatically padded from 4810 to 5120 to be a multiple of `config.attention_window`: 1024
 37% 95/258 [02:46<05:03,  1.86s/it]Input ids are automatically padded from 3744 to 4096 to be a multiple of `config.attention_window`: 1024
 39% 101/258 [02:57<04:57,  1.90s/it]Input ids are automatically padded from 3519 to 4096 to be a multiple of `config.attention_window`: 1024
 42% 108/258 [03:10<04:46,  1.91s/it]Input ids are automatically padded from 4785 to 5120 to be a multiple of `config.attention_window`: 1024
 42% 109/258 [03:12<04:46,  1.92s/it]Input ids are automatically padded from 3500 to 4096 to be a multiple of `config.attention_window`: 1024
 43% 111/258 [03:15<04:27,  1.82s/it]Input ids are automatically padded from 3638 to 4096 to be a multiple of `config.attention_window`: 1024
 45% 115/258 [03:23<04:28,  1.88s/it]Input ids are automatically padded from 2057 to 3072 to be a multiple of `config.attention_window`: 1024
 46% 119/258 [03:30<04:16,  1.85s/it]Input ids are automatically padded from 3529 to 4096 to be a multiple of `config.attention_window`: 1024
 47% 120/258 [03:31<03:59,  1.73s/it]Input ids are automatically padded from 3702 to 4096 to be a multiple of `config.attention_window`: 1024
 48% 123/258 [03:37<04:02,  1.80s/it]Input ids are automatically padded from 4709 to 5120 to be a multiple of `config.attention_window`: 1024
 48% 124/258 [03:38<04:07,  1.84s/it]Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024
 49% 126/258 [03:42<03:47,  1.73s/it]Input ids are automatically padded from 4958 to 5120 to be a multiple of `config.attention_window`: 1024
 49% 127/258 [03:43<03:52,  1.77s/it]Input ids are automatically padded from 3767 to 4096 to be a multiple of `config.attention_window`: 1024
 50% 128/258 [03:45<03:41,  1.70s/it]Input ids are automatically padded from 4643 to 5120 to be a multiple of `config.attention_window`: 1024
 51% 131/258 [03:51<03:54,  1.84s/it]Input ids are automatically padded from 1747 to 2048 to be a multiple of `config.attention_window`: 1024
 51% 132/258 [03:52<03:12,  1.53s/it]Input ids are automatically padded from 2980 to 3072 to be a multiple of `config.attention_window`: 1024
 52% 135/258 [03:57<03:26,  1.68s/it]Input ids are automatically padded from 3486 to 4096 to be a multiple of `config.attention_window`: 1024
 54% 139/258 [04:04<03:35,  1.81s/it]Input ids are automatically padded from 4747 to 5120 to be a multiple of `config.attention_window`: 1024
 55% 141/258 [04:08<03:38,  1.87s/it]Input ids are automatically padded from 4841 to 5120 to be a multiple of `config.attention_window`: 1024
 59% 152/258 [04:29<03:25,  1.94s/it]Input ids are automatically padded from 3472 to 4096 to be a multiple of `config.attention_window`: 1024
 59% 153/258 [04:30<03:10,  1.81s/it]Input ids are automatically padded from 4419 to 5120 to be a multiple of `config.attention_window`: 1024
 60% 154/258 [04:32<03:11,  1.84s/it]Input ids are automatically padded from 4063 to 4096 to be a multiple of `config.attention_window`: 1024
 60% 156/258 [04:36<03:03,  1.80s/it]Input ids are automatically padded from 3868 to 4096 to be a multiple of `config.attention_window`: 1024
 61% 158/258 [04:39<02:58,  1.78s/it]Input ids are automatically padded from 3360 to 4096 to be a multiple of `config.attention_window`: 1024
 62% 160/258 [04:43<02:54,  1.78s/it]Input ids are automatically padded from 3123 to 4096 to be a multiple of `config.attention_window`: 1024
 63% 162/258 [04:46<02:48,  1.75s/it]Input ids are automatically padded from 4232 to 5120 to be a multiple of `config.attention_window`: 1024
 64% 164/258 [04:50<02:51,  1.82s/it]Input ids are automatically padded from 511 to 1024 to be a multiple of `config.attention_window`: 1024
 65% 167/258 [04:54<02:33,  1.69s/it]Input ids are automatically padded from 2641 to 3072 to be a multiple of `config.attention_window`: 1024
 65% 168/258 [04:55<02:17,  1.52s/it]Input ids are automatically padded from 3150 to 4096 to be a multiple of `config.attention_window`: 1024
 67% 174/258 [05:06<02:36,  1.86s/it]Input ids are automatically padded from 4643 to 5120 to be a multiple of `config.attention_window`: 1024
 68% 176/258 [05:10<02:34,  1.88s/it]Input ids are automatically padded from 3486 to 4096 to be a multiple of `config.attention_window`: 1024
 69% 177/258 [05:12<02:23,  1.77s/it]Input ids are automatically padded from 2057 to 3072 to be a multiple of `config.attention_window`: 1024
 70% 180/258 [05:17<02:18,  1.78s/it]Input ids are automatically padded from 3702 to 4096 to be a multiple of `config.attention_window`: 1024
 71% 183/258 [05:22<02:15,  1.80s/it]Input ids are automatically padded from 4785 to 5120 to be a multiple of `config.attention_window`: 1024
 72% 186/258 [05:28<02:17,  1.91s/it]Input ids are automatically padded from 3500 to 4096 to be a multiple of `config.attention_window`: 1024
 74% 191/258 [05:37<02:07,  1.91s/it]Input ids are automatically padded from 2641 to 3072 to be a multiple of `config.attention_window`: 1024
 76% 195/258 [05:44<01:57,  1.86s/it]Input ids are automatically padded from 3472 to 4096 to be a multiple of `config.attention_window`: 1024
 76% 196/258 [05:46<01:48,  1.75s/it]Input ids are automatically padded from 4173 to 5120 to be a multiple of `config.attention_window`: 1024
 77% 199/258 [05:52<01:50,  1.87s/it]Input ids are automatically padded from 4841 to 5120 to be a multiple of `config.attention_window`: 1024
 78% 201/258 [05:55<01:47,  1.89s/it]Input ids are automatically padded from 4232 to 5120 to be a multiple of `config.attention_window`: 1024
 79% 204/258 [06:01<01:43,  1.91s/it]Input ids are automatically padded from 3123 to 4096 to be a multiple of `config.attention_window`: 1024
 81% 210/258 [06:12<01:30,  1.89s/it]Input ids are automatically padded from 511 to 1024 to be a multiple of `config.attention_window`: 1024
 82% 212/258 [06:14<01:12,  1.58s/it]Input ids are automatically padded from 3868 to 4096 to be a multiple of `config.attention_window`: 1024
 85% 220/258 [06:29<01:11,  1.89s/it]Input ids are automatically padded from 4709 to 5120 to be a multiple of `config.attention_window`: 1024
 88% 226/258 [06:41<01:01,  1.93s/it]Input ids are automatically padded from 4810 to 5120 to be a multiple of `config.attention_window`: 1024
 88% 227/258 [06:43<00:59,  1.92s/it]Input ids are automatically padded from 2980 to 3072 to be a multiple of `config.attention_window`: 1024
 88% 228/258 [06:44<00:51,  1.70s/it]Input ids are automatically padded from 3638 to 4096 to be a multiple of `config.attention_window`: 1024
 89% 229/258 [06:46<00:47,  1.65s/it]Input ids are automatically padded from 3763 to 4096 to be a multiple of `config.attention_window`: 1024
 89% 230/258 [06:47<00:45,  1.62s/it]Input ids are automatically padded from 3744 to 4096 to be a multiple of `config.attention_window`: 1024
 90% 231/258 [06:49<00:42,  1.59s/it]Input ids are automatically padded from 3529 to 4096 to be a multiple of `config.attention_window`: 1024
 90% 232/258 [06:50<00:40,  1.55s/it]Input ids are automatically padded from 3519 to 4096 to be a multiple of `config.attention_window`: 1024
 91% 234/258 [06:54<00:39,  1.64s/it]Input ids are automatically padded from 4063 to 4096 to be a multiple of `config.attention_window`: 1024
 91% 235/258 [06:55<00:37,  1.61s/it]Input ids are automatically padded from 3767 to 4096 to be a multiple of `config.attention_window`: 1024
 93% 239/258 [07:02<00:34,  1.82s/it]Input ids are automatically padded from 4747 to 5120 to be a multiple of `config.attention_window`: 1024
 93% 241/258 [07:06<00:32,  1.89s/it]Input ids are automatically padded from 3360 to 4096 to be a multiple of `config.attention_window`: 1024
 95% 245/258 [07:14<00:24,  1.89s/it]Input ids are automatically padded from 1747 to 2048 to be a multiple of `config.attention_window`: 1024
 97% 250/258 [07:22<00:14,  1.85s/it]Input ids are automatically padded from 4419 to 5120 to be a multiple of `config.attention_window`: 1024
 98% 253/258 [07:28<00:09,  1.88s/it]Input ids are automatically padded from 3150 to 4096 to be a multiple of `config.attention_window`: 1024
 99% 255/258 [07:31<00:05,  1.81s/it]Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024
100% 257/258 [07:34<00:01,  1.69s/it]Input ids are automatically padded from 4958 to 5120 to be a multiple of `config.attention_window`: 1024
{'train_runtime': 456.7722, 'train_samples_per_second': 0.565, 'epoch': 3.0}
100% 258/258 [07:36<00:00,  1.77s/it]
Saving model checkpoint to /content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16
Configuration saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16/config.json
Model weights saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16/pytorch_model.bin
02/17/2021 07:12:19 - INFO - __main__ -   ***** Train results *****
02/17/2021 07:12:19 - INFO - __main__ -     epoch = 3.0
02/17/2021 07:12:19 - INFO - __main__ -     train_runtime = 456.7722
02/17/2021 07:12:19 - INFO - __main__ -     train_samples_per_second = 0.565


Running Evaluation Script
0it [00:00, ?it/s]Input ids are automatically padded from 4159 to 5120 to be a multiple of `config.attention_window`: 1024
2it [00:09,  4.67s/it]Input ids are automatically padded from 3050 to 3072 to be a multiple of `config.attention_window`: 1024
3it [00:14,  4.86s/it]Input ids are automatically padded from 3403 to 4096 to be a multiple of `config.attention_window`: 1024
5it [00:23,  4.73s/it]Input ids are automatically padded from 3791 to 4096 to be a multiple of `config.attention_window`: 1024
6it [00:28,  4.59s/it]Input ids are automatically padded from 2625 to 3072 to be a multiple of `config.attention_window`: 1024
7it [00:32,  4.50s/it]Input ids are automatically padded from 4127 to 5120 to be a multiple of `config.attention_window`: 1024
8it [00:37,  4.59s/it]Input ids are automatically padded from 1889 to 2048 to be a multiple of `config.attention_window`: 1024
11it [00:51,  4.74s/it]Input ids are automatically padded from 4140 to 5120 to be a multiple of `config.attention_window`: 1024
13it [01:00,  4.63s/it]Input ids are automatically padded from 3601 to 4096 to be a multiple of `config.attention_window`: 1024
16it [01:16,  4.78s/it]
Evaluation Completed
Evaluation results saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16/16-test_results.csv
Evaluation scores saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_max_st_6144_550_summ_350_500_split_86_16/evaluation_scores.txt
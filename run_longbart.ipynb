{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run-longbart.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R7zoZ4CFqBwm",
        "2R67ClV7pg20",
        "d-fauWiVptTV"
      ],
      "mount_file_id": "1JJ6VKjDcSfUmfVaF2MVraklZY82RF_T2",
      "authorship_tag": "ABX9TyNyeJ1iLkYyhuNGkhAvV/Eg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsi319/LongFormer-Models/blob/main/run_longbart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arTxdl05mvtK",
        "outputId": "b28187b2-0ce3-4c7a-d1de-61984778ad80"
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "if path.exists(\"/content/drive/MyDrive/longformers/longbart\")==True:\n",
        "  %rm -r /content/drive/MyDrive/longformers/longbart\n",
        "  print(\"Deleted old repo\")\n",
        "!git clone https://github.com/nsi319/LongFormer-Models.git /content/drive/MyDrive/longformers/longbart \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/drive/MyDrive/longformers/longbart'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 17 (delta 5), reused 13 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xDWbkzunVq9",
        "outputId": "c475466c-d3b4-458a-bb3b-308971f7f210"
      },
      "source": [
        "!pip install transformers==2.11.0 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.11.0 in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.1.95)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1A3pUaQpV8-",
        "outputId": "785ca6a1-5c35-4315-d339-e643b8cf06aa"
      },
      "source": [
        "!python /content/drive/MyDrive/longformers/longbart/bart-longformer/bart_to_longbart.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-12 19:13:45.121763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: bart_to_longbart.py [-h] --save_model_path SAVE_MODEL_PATH\n",
            "                           [--base_model BASE_MODEL]\n",
            "                           [--tokenizer_name_or_path TOKENIZER_NAME_OR_PATH]\n",
            "                           [--max_length MAX_LENGTH]\n",
            "                           [--attention_window ATTENTION_WINDOW]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --save_model_path SAVE_MODEL_PATH\n",
            "                        The path to save the converted model\n",
            "  --base_model BASE_MODEL\n",
            "                        The name or path of the base model you want to convert\n",
            "  --tokenizer_name_or_path TOKENIZER_NAME_OR_PATH\n",
            "                        The name or path of the tokenizer\n",
            "  --max_length MAX_LENGTH\n",
            "                        Maximum encoder positions\n",
            "  --attention_window ATTENTION_WINDOW\n",
            "                        Attention window size for longformer self attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L3il3Y4nIpK",
        "outputId": "efa2a030-02c0-4f2d-d014-c6eed3ccf563"
      },
      "source": [
        "!python /content/drive/MyDrive/longformers/longbart/bart-longformer/bart_to_longbart.py \\\n",
        "    --base_model facebook/bart-large \\\n",
        "    --tokenizer_name_or_path facebook/bart-large \\\n",
        "    --save_model_path /content/drive/MyDrive/longformers/models/new-long-bart-large-4096 \\\n",
        "    --attention_window 512 \\\n",
        "    --max_length 4096 \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-13 06:05:29.524289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "ModelArguments(save_model_path='/content/drive/MyDrive/longformers/models/new-long-bart-large-4096', base_model='facebook/bart-large', tokenizer_name_or_path='facebook/bart-large', max_length=4096, attention_window=512)\n",
            "<class '__main__.ModelArguments'>\n",
            "INFO:filelock:Lock 140287796671040 acquired on /root/.cache/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.8b65d3b9a47e96c1909d807f7e7f41dd1ed95092b139965be7b914aa4fb5fd08.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmphkz7i4zo\n",
            "Downloading: 100% 1.52k/1.52k [00:00<00:00, 1.09MB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json in cache at /root/.cache/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.8b65d3b9a47e96c1909d807f7e7f41dd1ed95092b139965be7b914aa4fb5fd08\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.8b65d3b9a47e96c1909d807f7e7f41dd1ed95092b139965be7b914aa4fb5fd08\n",
            "INFO:filelock:Lock 140287796671040 released on /root/.cache/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.8b65d3b9a47e96c1909d807f7e7f41dd1ed95092b139965be7b914aa4fb5fd08.lock\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json from cache at /root/.cache/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.8b65d3b9a47e96c1909d807f7e7f41dd1ed95092b139965be7b914aa4fb5fd08\n",
            "INFO:transformers.configuration_utils:Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_max_position_embeddings\": 1024,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"encoder_max_position_embeddings\": 1024,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:filelock:Lock 140287796673056 acquired on /root/.cache/torch/transformers/2e7cae41bb1dd1f18e498ff4ff0ea85f7e9bc2b637439e2d95c485c5d5bdd579.5442efe9b99297b6484ff9378cc5af7f47a37d305cbec072c44eb329873e4fe6.lock\n",
            "INFO:transformers.file_utils:https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpdvkaba69\n",
            "Downloading: 100% 1.02G/1.02G [00:22<00:00, 45.2MB/s]\n",
            "INFO:transformers.file_utils:storing https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin in cache at /root/.cache/torch/transformers/2e7cae41bb1dd1f18e498ff4ff0ea85f7e9bc2b637439e2d95c485c5d5bdd579.5442efe9b99297b6484ff9378cc5af7f47a37d305cbec072c44eb329873e4fe6\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/2e7cae41bb1dd1f18e498ff4ff0ea85f7e9bc2b637439e2d95c485c5d5bdd579.5442efe9b99297b6484ff9378cc5af7f47a37d305cbec072c44eb329873e4fe6\n",
            "INFO:filelock:Lock 140287796673056 released on /root/.cache/torch/transformers/2e7cae41bb1dd1f18e498ff4ff0ea85f7e9bc2b637439e2d95c485c5d5bdd579.5442efe9b99297b6484ff9378cc5af7f47a37d305cbec072c44eb329873e4fe6.lock\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin from cache at /root/.cache/torch/transformers/2e7cae41bb1dd1f18e498ff4ff0ea85f7e9bc2b637439e2d95c485c5d5bdd579.5442efe9b99297b6484ff9378cc5af7f47a37d305cbec072c44eb329873e4fe6\n",
            "INFO:transformers.modeling_utils:Weights of BartForConditionalGeneration not initialized from pretrained model: ['final_logits_bias']\n",
            "INFO:filelock:Lock 140287780370864 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpbi8g8fn8\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 12.4MB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json in cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:filelock:Lock 140287780370864 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "INFO:filelock:Lock 140287780371032 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpkswq70q_\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 6.53MB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt in cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:filelock:Lock 140287780371032 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:__main__:saving model to /content/drive/MyDrive/longformers/models/new-long-bart-large-4096\n",
            "INFO:transformers.configuration_utils:Configuration saved in /content/drive/MyDrive/longformers/models/new-long-bart-large-4096/config.json\n",
            "INFO:transformers.modeling_utils:Model weights saved in /content/drive/MyDrive/longformers/models/new-long-bart-large-4096/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWtxxv4sGCi4"
      },
      "source": [
        "# Model Configuration:\n",
        "\"\"\"\n",
        " {\n",
        "  \"activation_dropout\": 0.1,\n",
        "  \"activation_function\": \"gelu\",\n",
        "  \"add_bias_logits\": false,\n",
        "  \"add_final_layer_norm\": false,\n",
        "  \"architectures\": [\n",
        "    \"BartForConditionalGeneration\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.1,\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"attention_window\": [\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512\n",
        "  ],\n",
        "  \"bos_token_id\": 0,\n",
        "  \"classif_dropout\": 0.0,\n",
        "  \"d_model\": 1024,\n",
        "  \"decoder_attention_heads\": 16,\n",
        "  \"decoder_ffn_dim\": 4096,\n",
        "  \"decoder_layerdrop\": 0.0,\n",
        "  \"decoder_layers\": 12,\n",
        "  \"decoder_max_position_embeddings\": 1024,\n",
        "  \"decoder_start_token_id\": 2,\n",
        "  \"dropout\": 0.1,\n",
        "  \"early_stopping\": true,\n",
        "  \"encoder_attention_heads\": 16,\n",
        "  \"encoder_ffn_dim\": 4096,\n",
        "  \"encoder_layerdrop\": 0.0,\n",
        "  \"encoder_layers\": 12,\n",
        "  \"encoder_max_position_embeddings\": 4096,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"gradient_checkpointing\": false,\n",
        "  \"id2label\": {\n",
        "    \"0\": \"LABEL_0\",\n",
        "    \"1\": \"LABEL_1\",\n",
        "    \"2\": \"LABEL_2\"\n",
        "  },\n",
        "  \"init_std\": 0.02,\n",
        "  \"is_encoder_decoder\": true,\n",
        "  \"label2id\": {\n",
        "    \"LABEL_0\": 0,\n",
        "    \"LABEL_1\": 1,\n",
        "    \"LABEL_2\": 2\n",
        "  },\n",
        "  \"max_position_embeddings\": 1024,\n",
        "  \"model_type\": \"bart\",\n",
        "  \"no_repeat_ngram_size\": 3,\n",
        "  \"normalize_before\": false,\n",
        "  \"normalize_embedding\": true,\n",
        "  \"num_beams\": 4,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"output_attentions\": true,\n",
        "  \"pad_token_id\": 1,\n",
        "  \"scale_embedding\": false,\n",
        "  \"static_position_embeddings\": false,\n",
        "  \"task_specific_params\": {\n",
        "    \"summarization\": {\n",
        "      \"length_penalty\": 1.0,\n",
        "      \"max_length\": 128,\n",
        "      \"min_length\": 12,\n",
        "      \"num_beams\": 4\n",
        "    },\n",
        "    \"summarization_cnn\": {\n",
        "      \"length_penalty\": 2.0,\n",
        "      \"max_length\": 142,\n",
        "      \"min_length\": 56,\n",
        "      \"num_beams\": 4\n",
        "    },\n",
        "    \"summarization_xsum\": {\n",
        "      \"length_penalty\": 1.0,\n",
        "      \"max_length\": 62,\n",
        "      \"min_length\": 11,\n",
        "      \"num_beams\": 6\n",
        "    }\n",
        "  },\n",
        "  \"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vlqECyDEdOf"
      },
      "source": [
        "# Test model with input text greater than 1024\n",
        "\n",
        "!python /content/drive/MyDrive/longformers/longbart/bart-longformer/test_longbart.py \\\n",
        "    --model_path /content/drive/MyDrive/longformers/models/long-bart-large-4096"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIz_J9SarZB8"
      },
      "source": [
        "# Testing phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7zoZ4CFqBwm"
      },
      "source": [
        "# **Bart Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy8JWlpppIsX"
      },
      "source": [
        "\"\"\" BART configuration \"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "BART_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    \"facebook/bart-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json\",\n",
        "    \"facebook/bart-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/config.json\",\n",
        "    \"facebook/bart-large-cnn\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/config.json\",\n",
        "    \"facebook/bart-large-xsum\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-xsum/config.json\",\n",
        "    \"facebook/mbart-large-en-ro\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json\",\n",
        "}\n",
        "\n",
        "\n",
        "class BartConfig(PretrainedConfig):\n",
        "    r\"\"\"\n",
        "        Configuration class for Bart. Parameters are renamed from the fairseq implementation\n",
        "    \"\"\"\n",
        "    model_type = \"bart\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        activation_dropout=0.0,\n",
        "        activation_function=\"gelu\",\n",
        "        vocab_size=50265,\n",
        "        d_model=1024,\n",
        "        encoder_ffn_dim=4096,\n",
        "        encoder_layers=12,\n",
        "        encoder_attention_heads=16,\n",
        "        decoder_ffn_dim=4096,\n",
        "        decoder_layers=12,\n",
        "        decoder_attention_heads=16,\n",
        "        encoder_layerdrop=0.0,\n",
        "        decoder_layerdrop=0.0,\n",
        "        attention_dropout=0.0,\n",
        "        dropout=0.1,\n",
        "        max_position_embeddings=1024,\n",
        "        encoder_max_position_embeddings=None,\t\n",
        "        decoder_max_position_embeddings=None,\n",
        "        init_std=0.02,\n",
        "        classifier_dropout=0.0,\n",
        "        num_labels=3,\n",
        "        is_encoder_decoder=True,\n",
        "        pad_token_id=1,\n",
        "        bos_token_id=0,\n",
        "        eos_token_id=2,\n",
        "        normalize_before=False,\n",
        "        add_final_layer_norm=False,\n",
        "        scale_embedding=False,\n",
        "        normalize_embedding=True,\n",
        "        static_position_embeddings=False,\n",
        "        add_bias_logits=False,\n",
        "        gradient_checkpointing=False,\n",
        "        **common_kwargs\n",
        "    ):\n",
        "        r\"\"\"\n",
        "            :class:`~transformers.BartConfig` is the configuration class for `BartModel`.\n",
        "            Examples:\n",
        "                config = BartConfig.from_pretrained('bart-large')\n",
        "                model = BartModel(config)\n",
        "        \"\"\"\n",
        "        if \"hidden_size\" in common_kwargs:\n",
        "            raise ValueError(\"hidden size is called d_model\")\n",
        "        super().__init__(\n",
        "            num_labels=num_labels,\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            is_encoder_decoder=is_encoder_decoder,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model  # encoder_embed_dim and decoder_embed_dim\n",
        "        self.encoder_ffn_dim = encoder_ffn_dim\n",
        "        self.encoder_layers = self.num_hidden_layers = encoder_layers\n",
        "        self.encoder_attention_heads = encoder_attention_heads\n",
        "        self.encoder_layerdrop = encoder_layerdrop\n",
        "        self.decoder_layerdrop = decoder_layerdrop\n",
        "        self.decoder_ffn_dim = decoder_ffn_dim\n",
        "        self.decoder_layers = decoder_layers\n",
        "        self.decoder_attention_heads = decoder_attention_heads\n",
        "        self.init_std = init_std  # Normal(0, this parameter)\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.encoder_max_position_embeddings = encoder_max_position_embeddings if encoder_max_position_embeddings else max_position_embeddings\t\n",
        "        self.decoder_max_position_embeddings = decoder_max_position_embeddings if decoder_max_position_embeddings else max_position_embeddings\n",
        "\n",
        "        # Params introduced for Mbart\n",
        "        self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n",
        "        self.normalize_embedding = normalize_embedding  # True for mbart, False otherwise\n",
        "        self.normalize_before = normalize_before  # combo of fairseq's encoder_ and decoder_normalize_before\n",
        "        self.add_final_layer_norm = add_final_layer_norm\n",
        "\n",
        "        # Params introduced for Marian\n",
        "        self.add_bias_logits = add_bias_logits\n",
        "        self.static_position_embeddings = static_position_embeddings\n",
        "\n",
        "        # 3 Types of Dropout\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Classifier stuff\n",
        "        self.classif_dropout = classifier_dropout\n",
        "        \n",
        "        # gradient_checkpointing\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        self.output_attentions = True\n",
        "\n",
        "    @property\n",
        "    def num_attention_heads(self) -> int:\n",
        "        return self.encoder_attention_heads\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.d_model\n",
        "\n",
        "    def is_valid_mbart(self) -> bool:\n",
        "        \"\"\"Is the configuration aligned with the MBART paper.\"\"\"\n",
        "        if self.normalize_before and self.add_final_layer_norm and self.scale_embedding:\n",
        "            return True\n",
        "        if self.normalize_before or self.add_final_layer_norm or self.scale_embedding:\n",
        "            logger.info(\"This configuration is a mixture of MBART and BART settings\")\n",
        "        return False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R67ClV7pg20"
      },
      "source": [
        "# **BartForConditionalGeneration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeTo497To9IC"
      },
      "source": [
        "\"\"\"PyTorch BART model, ported from the fairseq repo.\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_callable\n",
        "from transformers.modeling_utils import PreTrainedModel, create_position_ids_from_input_ids\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/bart-large\",\n",
        "    \"facebook/bart-large-mnli\",\n",
        "    \"facebook/bart-large-cnn\",\n",
        "    \"facebook/bart-large-xsum\",\n",
        "    \"facebook/mbart-large-en-ro\",\n",
        "    # See all BART models at https://huggingface.co/models?filter=bart\n",
        "]\n",
        "\n",
        "\n",
        "BART_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class. Use it as a regular PyTorch Module and\n",
        "    refer to the PyTorch documentation for all matters related to general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
        "            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n",
        "\n",
        "\"\"\"\n",
        "BART_GENERATION_EXAMPLE = r\"\"\"\n",
        "    Examples::\n",
        "\n",
        "        from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "        # see ``examples/summarization/bart/evaluate_cnn.py`` for a longer example\n",
        "        model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
        "        tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
        "        ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "        inputs = tokenizer.batch_encode_plus([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
        "        # Generate Summary\n",
        "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
        "        print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "               Indices of input sequence tokens in the vocabulary. Use BartTokenizer.encode to produce them.\n",
        "            Padding will be ignored by default should you provide it.\n",
        "            Indices can be obtained using :class:`transformers.BartTokenizer.encode(text)`.\n",
        "        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Mask to avoid performing attention on padding token indices in input_ids.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`, defaults to :obj:`None`):\n",
        "            Tuple consists of (`last_hidden_state`, `optional`: `hidden_states`, `optional`: `attentions`)\n",
        "            `last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`, defaults to :obj:`None`) is a sequence of hidden-states at the output of the last layer of the encoder.\n",
        "            Used in the cross-attention of the decoder.\n",
        "        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Provide for translation and summarization training. By default, the model will create this tensor by shifting the input_ids right, following the paper.\n",
        "        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`, defaults to :obj:`None`):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in decoder_input_ids. Causal mask will also be used by default.\n",
        "            If you want to change padding behavior, you should read :func:`~transformers.modeling_bart._prepare_decoder_inputs` and modify.\n",
        "            See diagram 1 in the paper for more info on the default strategy\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def invert_mask(attention_mask):\n",
        "    assert attention_mask.dim() == 2\n",
        "    return attention_mask.eq(0)\n",
        "\n",
        "\n",
        "def _prepare_bart_decoder_inputs(\n",
        "    config, input_ids, decoder_input_ids=None, decoder_padding_mask=None, causal_mask_dtype=torch.float32\n",
        "):\n",
        "    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n",
        "    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n",
        "    Note: this is not called during generation\n",
        "    \"\"\"\n",
        "    pad_token_id = config.pad_token_id\n",
        "    if decoder_input_ids is None:\n",
        "        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n",
        "    bsz, tgt_len = decoder_input_ids.size()\n",
        "    if decoder_padding_mask is None:\n",
        "        decoder_padding_mask = make_padding_mask(decoder_input_ids, pad_token_id)\n",
        "    else:\n",
        "        decoder_padding_mask = invert_mask(decoder_padding_mask)\n",
        "    causal_mask = torch.triu(fill_with_neg_inf(torch.zeros(tgt_len, tgt_len)), 1).to(\n",
        "        dtype=causal_mask_dtype, device=decoder_input_ids.device\n",
        "    )\n",
        "    return decoder_input_ids, decoder_padding_mask, causal_mask\n",
        "\n",
        "\n",
        "class PretrainedBartModel(PreTrainedModel):\n",
        "    config_class = BartConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs\n",
        "\n",
        "\n",
        "def _make_linear_from_emb(emb):\n",
        "    vocab_size, emb_size = emb.weight.shape\n",
        "    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "    lin_layer.weight.data = emb.weight.data\n",
        "    return lin_layer\n",
        "\n",
        "\n",
        "# Helper Functions, mostly for making masks\n",
        "def _check_shapes(shape_1, shape2):\n",
        "    if shape_1 != shape2:\n",
        "        raise AssertionError(\"shape mismatch: {} != {}\".format(shape_1, shape2))\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
        "    prev_output_tokens = input_ids.clone()\n",
        "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "    return prev_output_tokens\n",
        "\n",
        "\n",
        "def make_padding_mask(input_ids, padding_idx=1):\n",
        "    \"\"\"True for pad tokens\"\"\"\n",
        "    padding_mask = input_ids.eq(padding_idx)\n",
        "    if not padding_mask.any():\n",
        "        padding_mask = None\n",
        "    return padding_mask\n",
        "\n",
        "\n",
        "# Helper Modules\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.self_attn = SelfAttention(\n",
        "            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.normalize_before = config.normalize_before\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
        "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
        "            for t_tgt, t_src is excluded (or masked out), =0 means it is\n",
        "            included in attention\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        x, attn_weights = self.self_attn(\n",
        "            query=x, key=x, key_padding_mask=encoder_padding_mask, need_weights=self.output_attentions\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return (x, attn_weights)\n",
        "\n",
        "\n",
        "class BartEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n",
        "    is a :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        embed_dim = embed_tokens.embedding_dim\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.encoder_max_position_embeddings\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.encoder_max_position_embeddings, embed_dim, self.padding_idx\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.encoder_max_position_embeddings, embed_dim, self.padding_idx,\n",
        "            )\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = LayerNorm(embed_dim) if config.normalize_embedding else nn.Identity()\n",
        "        # mbart has one extra layer_norm\n",
        "        self.layer_norm = LayerNorm(config.d_model) if config.normalize_before else None\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (LongTensor): tokens in the source language of shape\n",
        "                `(batch, src_len)`\n",
        "            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n",
        "        Returns:\n",
        "            Tuple comprised of:\n",
        "                - **x** (Tensor): the last encoder layer's output of\n",
        "                  shape `(src_len, batch, embed_dim)`\n",
        "                - **encoder_states** (List[Tensor]): all intermediate\n",
        "                  hidden states of shape `(src_len, batch, embed_dim)`.\n",
        "                  Only populated if *self.output_hidden_states:* is True.\n",
        "                - **all_attentions** (List[Tensor]): Attention weights for each layer.\n",
        "                During training might not be of length n_layers because of layer dropout.\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = invert_mask(attention_mask)\n",
        "\n",
        "        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        embed_pos = self.embed_positions(input_ids)\n",
        "        x = inputs_embeds + embed_pos\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        encoder_states, all_attentions = [], []\n",
        "        for encoder_layer in self.layers:\n",
        "            if self.output_hidden_states:\n",
        "                encoder_states.append(x)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                attn = None\n",
        "            else:\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False):\n",
        "                    x, attn = torch.utils.checkpoint.checkpoint(\n",
        "                        encoder_layer,\n",
        "                        x,\n",
        "                        attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    x, attn = encoder_layer(x, attention_mask)\n",
        "\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions.append(attn)\n",
        "\n",
        "        if self.layer_norm:\n",
        "            x = self.layer_norm(x)\n",
        "        if self.output_hidden_states:\n",
        "            encoder_states.append(x)\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        encoder_states = [hidden_state.transpose(0, 1) for hidden_state in encoder_states]\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, encoder_states, all_attentions\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = SelfAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.normalize_before = config.normalize_before\n",
        "\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = SelfAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attn_mask=None,\n",
        "        layer_state=None,\n",
        "        causal_mask=None,\n",
        "        decoder_padding_mask=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        residual = x\n",
        "\n",
        "        if layer_state is None:\n",
        "            layer_state = {}\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        # Self Attention\n",
        "\n",
        "        x, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            layer_state=layer_state,  # adds keys to layer state\n",
        "            key_padding_mask=decoder_padding_mask,\n",
        "            attn_mask=causal_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        # Cross attention\n",
        "        residual = x\n",
        "        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n",
        "        if self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "        x, _ = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_hidden_states,\n",
        "            key_padding_mask=encoder_attn_mask,\n",
        "            layer_state=layer_state,  # mutates layer state\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            layer_state,\n",
        "        )  # just self_attn weights for now, following t5, layer_state = cache for decoding\n",
        "\n",
        "\n",
        "class BartDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n",
        "    is a :class:`DecoderLayer`.\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, config.pad_token_id\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, self.padding_idx,\n",
        "            )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderLayer(config) for _ in range(config.decoder_layers)]\n",
        "        )  # type: List[DecoderLayer]\n",
        "        self.layernorm_embedding = LayerNorm(config.d_model) if config.normalize_embedding else nn.Identity()\n",
        "        self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_padding_mask,\n",
        "        decoder_padding_mask,\n",
        "        decoder_causal_mask,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        **unused,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Includes several features from \"Jointly Learning to Align and\n",
        "        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n",
        "\n",
        "        Args:\n",
        "            input_ids (LongTensor): previous decoder outputs of shape\n",
        "                `(batch, tgt_len)`, for teacher forcing\n",
        "            encoder_hidden_states: output from the encoder, used for\n",
        "                encoder-side attention\n",
        "            encoder_padding_mask: for ignoring pad tokens\n",
        "            decoder_cached_states (dict or None): dictionary used for storing state during generation\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
        "                - hidden states\n",
        "                - attentions\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if encoder_padding_mask is not None:\n",
        "            encoder_padding_mask = invert_mask(encoder_padding_mask)\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_ids, use_cache=use_cache)\n",
        "\n",
        "        if use_cache:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "            positions = positions[:, -1:]  # happens after we embed them\n",
        "            # assert input_ids.ne(self.padding_idx).any()\n",
        "\n",
        "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        x += positions\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = ()\n",
        "        all_self_attns = ()\n",
        "        next_decoder_cache = []\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states += (x,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            layer_state = decoder_cached_states[idx] if decoder_cached_states is not None else None\n",
        "\n",
        "            x, layer_self_attn, layer_past = decoder_layer(\n",
        "                x,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attn_mask=encoder_padding_mask,\n",
        "                decoder_padding_mask=decoder_padding_mask,\n",
        "                layer_state=layer_state,\n",
        "                causal_mask=decoder_causal_mask,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache.append(layer_past.copy())\n",
        "\n",
        "            if self.layer_norm and (idx == len(self.layers) - 1):  # last layer of mbart\n",
        "                x = self.layer_norm(x)\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_self_attn,)\n",
        "\n",
        "        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        all_hidden_states = [hidden_state.transpose(0, 1) for hidden_state in all_hidden_states]\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        if use_cache:\n",
        "            next_cache = ((encoder_hidden_states, encoder_padding_mask), next_decoder_cache)\n",
        "        else:\n",
        "            next_cache = None\n",
        "        return x, next_cache, all_hidden_states, list(all_self_attns)\n",
        "\n",
        "\n",
        "def _reorder_buffer(attn_cache, new_order):\n",
        "    for k, input_buffer_k in attn_cache.items():\n",
        "        if input_buffer_k is not None:\n",
        "            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n",
        "    return attn_cache\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n",
        "\n",
        "    def _shape(self, tensor, dim_0, bsz):\n",
        "        return tensor.contiguous().view(dim_0, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query,\n",
        "        key: Optional[Tensor],\n",
        "        key_padding_mask: Optional[Tensor] = None,\n",
        "        layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "        need_weights=False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n",
        "        static_kv: bool = self.encoder_decoder_attention\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        # get here for encoder decoder cause of static_kv\n",
        "        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n",
        "            saved_state = layer_state.get(self.cache_key, {})\n",
        "            if \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute key and value if they are static\n",
        "                if static_kv:\n",
        "                    key = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "            layer_state = {}\n",
        "\n",
        "        q = self.q_proj(query) * self.scaling\n",
        "        if static_kv:\n",
        "            if key is None:\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self._shape(q, tgt_len, bsz)\n",
        "        if k is not None:\n",
        "            k = self._shape(k, -1, bsz)\n",
        "        if v is not None:\n",
        "            v = self._shape(v, -1, bsz)\n",
        "\n",
        "        if saved_state is not None:\n",
        "            k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)\n",
        "\n",
        "        # Update cache\n",
        "        layer_state[self.cache_key] = {\n",
        "            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_key_padding_mask\": key_padding_mask if not static_kv else None,\n",
        "        }\n",
        "\n",
        "        assert k is not None\n",
        "        src_len = k.size(1)\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attn_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "        assert key_padding_mask is None or key_padding_mask.size()[:2] == (bsz, src_len,)\n",
        "\n",
        "        if key_padding_mask is not None:  # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training,)\n",
        "\n",
        "        assert v is not None\n",
        "        attn_output = torch.bmm(attn_probs, v)\n",
        "        assert attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights = None\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv, bsz):\n",
        "        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "        if \"prev_key\" in saved_state:\n",
        "            _prev_key = saved_state[\"prev_key\"]\n",
        "            assert _prev_key is not None\n",
        "            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                k = prev_key\n",
        "            else:\n",
        "                assert k is not None\n",
        "                k = torch.cat([prev_key, k], dim=1)\n",
        "        if \"prev_value\" in saved_state:\n",
        "            _prev_value = saved_state[\"prev_value\"]\n",
        "            assert _prev_value is not None\n",
        "            prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                v = prev_value\n",
        "            else:\n",
        "                assert v is not None\n",
        "                v = torch.cat([prev_value, v], dim=1)\n",
        "        assert k is not None and v is not None\n",
        "        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\"prev_key_padding_mask\", None)\n",
        "        key_padding_mask = self._cat_prev_key_padding_mask(\n",
        "            key_padding_mask, prev_key_padding_mask, bsz, k.size(1), static_kv\n",
        "        )\n",
        "        return k, v, key_padding_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_prev_key_padding_mask(\n",
        "        key_padding_mask: Optional[Tensor],\n",
        "        prev_key_padding_mask: Optional[Tensor],\n",
        "        batch_size: int,\n",
        "        src_len: int,\n",
        "        static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None:\n",
        "            if static_kv:\n",
        "                new_key_padding_mask = prev_key_padding_mask\n",
        "            else:\n",
        "                new_key_padding_mask = torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)\n",
        "\n",
        "        elif key_padding_mask is not None:\n",
        "            filler = torch.zeros(\n",
        "                batch_size,\n",
        "                src_len - key_padding_mask.size(1),\n",
        "                dtype=key_padding_mask.dtype,\n",
        "                device=key_padding_mask.device,\n",
        "            )\n",
        "            new_key_padding_mask = torch.cat([filler, key_padding_mask], dim=1)\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "\n",
        "class BartClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    # This can trivially be shared with RobertaClassificationHead\n",
        "\n",
        "    def __init__(\n",
        "        self, input_dim, inner_dim, num_classes, pooler_dropout,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inner_dim)\n",
        "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    Padding ids are ignored by either offsetting based on padding_idx\n",
        "    or by setting padding_idx to None and ensuring that the appropriate\n",
        "    position ids are passed to the forward function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_embeddings: int, embedding_dim: int, padding_idx: int,\n",
        "    ):\n",
        "        # if padding_idx is specified then offset the embedding ids by\n",
        "        # this index and adjust num_embeddings appropriately\n",
        "        assert padding_idx is not None\n",
        "        num_embeddings += padding_idx + 1  # WHY?\n",
        "        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "    def forward(self, input, use_cache=False):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        if use_cache:  # the position is our current step in the decoded sequence\n",
        "            pos = int(self.padding_idx + input.size(1))\n",
        "            positions = input.data.new(1, 1).fill_(pos)\n",
        "        else:\n",
        "            positions = create_position_ids_from_input_ids(input, self.padding_idx)\n",
        "        return super().forward(positions)\n",
        "\n",
        "\n",
        "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            from apex.normalization import FusedLayerNorm\n",
        "\n",
        "            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n",
        "        except ImportError:\n",
        "            pass\n",
        "    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n",
        "\n",
        "\n",
        "def fill_with_neg_inf(t):\n",
        "    \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n",
        "    return t.float().fill_(float(\"-inf\")).type_as(t)\n",
        "\n",
        "\n",
        "def _filter_out_falsey_values(tup) -> Tuple:\n",
        "    \"\"\"Remove entries that are None or [] from an iterable.\"\"\"\n",
        "    return tuple(x for x in tup if isinstance(x, torch.Tensor) or x)\n",
        "\n",
        "\n",
        "# Public API\n",
        "def _get_shape(t):\n",
        "    return getattr(t, \"shape\", None)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare BART Model outputting raw hidden-states without any specific head on top.\", BART_START_DOCSTRING,\n",
        ")\n",
        "class BartModel(PretrainedBartModel):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.encoder = BartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        encoder_outputs: Optional[Tuple] = None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "    ):\n",
        "\n",
        "        # make masks if user doesn't supply\n",
        "        if not use_cache:\n",
        "            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n",
        "                self.config,\n",
        "                input_ids,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                decoder_padding_mask=decoder_attention_mask,\n",
        "                causal_mask_dtype=self.shared.weight.dtype,\n",
        "            )\n",
        "        else:\n",
        "            decoder_padding_mask, causal_mask = None, None\n",
        "\n",
        "        assert decoder_input_ids is not None\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        assert isinstance(encoder_outputs, tuple)\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            decoder_input_ids,\n",
        "            encoder_outputs[0],\n",
        "            attention_mask,\n",
        "            decoder_padding_mask,\n",
        "            decoder_causal_mask=causal_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        # Attention and hidden_states will be [] or None if they aren't needed\n",
        "        decoder_outputs: Tuple = _filter_out_falsey_values(decoder_outputs)\n",
        "        assert isinstance(decoder_outputs[0], torch.Tensor)\n",
        "        encoder_outputs: Tuple = _filter_out_falsey_values(encoder_outputs)\n",
        "        return decoder_outputs + encoder_outputs\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.shared)  # make it on the fly\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The BART Model with a language modeling head. Can be used for summarization.\",\n",
        "    BART_START_DOCSTRING + BART_GENERATION_EXAMPLE,\n",
        ")\n",
        "class BartForConditionalGeneration(PretrainedBartModel):\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        base_model = BartModel(config)\n",
        "        self.model = base_model\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        old_num_tokens = self.model.shared.num_embeddings\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self.model.shared = new_embeddings\n",
        "        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        lm_labels=None,\n",
        "        use_cache=False,\n",
        "        **unused\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        lm_labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n",
        "            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n",
        "            with labels\n",
        "            in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:\n",
        "        masked_lm_loss (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Masked language modeling loss.\n",
        "        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`)\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "            # Mask filling only works for bart-large\n",
        "            from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "            tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
        "            TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
        "            model = BartForConditionalGeneration.from_pretrained('bart-large')\n",
        "            input_ids = tokenizer.batch_encode_plus([TXT], return_tensors='pt')['input_ids']\n",
        "            logits = model(input_ids)[0]\n",
        "            masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "            probs = logits[0, masked_index].softmax(dim=0)\n",
        "            values, predictions = probs.topk(5)\n",
        "            tokenizer.decode(predictions).split()\n",
        "            # ['good', 'great', 'all', 'really', 'very']\n",
        "        \"\"\"\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)\n",
        "        outputs = (lm_logits,) + outputs[1:]  # Add cache, hidden states and attention if they are here\n",
        "        if lm_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # TODO(SS): do we need to ignore pad tokens in lm_labels?\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), lm_labels.view(-1))\n",
        "            outputs = (masked_lm_loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def prepare_inputs_for_generation(self, decoder_input_ids, past, attention_mask, use_cache, **kwargs):\n",
        "        assert past is not None, \"past has to be defined for encoder_outputs\"\n",
        "\n",
        "        # first step, decoder_cached_states are empty\n",
        "        if not past[1]:\n",
        "            encoder_outputs, decoder_cached_states = past, None\n",
        "        else:\n",
        "            encoder_outputs, decoder_cached_states = past\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"decoder_cached_states\": decoder_cached_states,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_logits_for_generation(self, logits, cur_len, max_length):\n",
        "        if cur_len == 1:\n",
        "            self._force_token_ids_generation(logits, self.config.bos_token_id)\n",
        "        if cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
        "            self._force_token_ids_generation(logits, self.config.eos_token_id)\n",
        "        return logits\n",
        "\n",
        "    def _force_token_ids_generation(self, scores, token_ids) -> None:\n",
        "        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\n",
        "        if isinstance(token_ids, int):\n",
        "            token_ids = [token_ids]\n",
        "        all_but_token_ids_mask = torch.tensor(\n",
        "            [x for x in range(self.config.vocab_size) if x not in token_ids],\n",
        "            dtype=torch.long,\n",
        "            device=next(self.parameters()).device,\n",
        "        )\n",
        "        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n",
        "        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        ((enc_out, enc_mask), decoder_cached_states) = past\n",
        "        reordered_past = []\n",
        "        for layer_past in decoder_cached_states:\n",
        "            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n",
        "            layer_past_new = {\n",
        "                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n",
        "            }\n",
        "            reordered_past.append(layer_past_new)\n",
        "\n",
        "        new_enc_out = enc_out if enc_out is None else enc_out.index_select(0, beam_idx)\n",
        "        new_enc_mask = enc_mask if enc_mask is None else enc_mask.index_select(0, beam_idx)\n",
        "\n",
        "        past = ((new_enc_out, new_enc_mask), reordered_past)\n",
        "        return past\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.encoder\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks. \"\"\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartForSequenceClassification(PretrainedBartModel):\n",
        "    def __init__(self, config: BartConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.model = BartModel(config)\n",
        "        self.classification_head = BartClassificationHead(\n",
        "            config.d_model, config.d_model, config.num_labels, config.classif_dropout,\n",
        "        )\n",
        "        self.model._init_weights(self.classification_head.dense)\n",
        "        self.model._init_weights(self.classification_head.out_proj)\n",
        "\n",
        "    @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BartConfig`) and inputs:\n",
        "            loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
        "                Classification loss (cross entropy)\n",
        "            logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
        "                Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "                Attentions weights after the attention softmax, used to compute the weighted average in the\n",
        "                self-attention\n",
        "                heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        from transformers import BartTokenizer, BartForSequenceClassification\n",
        "        import torch\n",
        "\n",
        "        tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
        "        model = BartForSequenceClassification.from_pretrained('bart-large')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\",\n",
        "        add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        \"\"\"\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "        )\n",
        "        x = outputs[0]  # last hidden state\n",
        "        eos_mask = input_ids.eq(self.config.eos_token_id)\n",
        "        if len(torch.unique(eos_mask.sum(1))) > 1:\n",
        "            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n",
        "        sentence_representation = x[eos_mask, :].view(x.size(0), -1, x.size(-1))[:, -1, :]\n",
        "        logits = self.classification_head(sentence_representation)\n",
        "        # Prepend logits\n",
        "        outputs = (logits,) + outputs[1:]  # Add hidden states and attention if they are here\n",
        "        if labels is not None:  # prepend loss to output,\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        if embedding_dim % 2 != 0:\n",
        "            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        \"\"\"Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.\n",
        "            The cos features are in the 2nd half of the vector. [dim // 2:]\n",
        "        \"\"\"\n",
        "        n_pos, dim = out.shape\n",
        "        position_enc = np.array(\n",
        "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
        "        )\n",
        "        out[:, 0 : dim // 2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))  # This line breaks for odd n_pos\n",
        "        out[:, dim // 2 :] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        out.detach_()\n",
        "        out.requires_grad = False\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids, use_cache=False):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        if use_cache:\n",
        "            positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n",
        "        else:\n",
        "            # starts at 0, ends at 1-seq_len\n",
        "            positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
        "        return super().forward(positions)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-fauWiVptTV"
      },
      "source": [
        "# **LongBartForConditionalGeneration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JexdbZbogOR"
      },
      "source": [
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from transformers.modeling_longformer import LongformerSelfAttention\n",
        "\n",
        "\n",
        "class LongBartForConditionalGeneration(BartForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.model.encoder.layers):\n",
        "            # replace the `modeling_bart.SelfAttention` object with `LongformerSelfAttention`\n",
        "            layer.self_attn = LongformerSelfAttentionForBart(config, layer_id=i)\n",
        "\n",
        "\n",
        "class LongformerSelfAttentionForBart(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.longformer_self_attn = LongformerSelfAttention(config, layer_id=layer_id)\n",
        "        self.output = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        query,\n",
        "        key: Optional[Tensor],\n",
        "        key_padding_mask: Optional[Tensor] = None,\n",
        "        layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "        need_weights=False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        \n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "\n",
        "        # LongformerSelfAttention expects this shape\n",
        "        query = query.view(bsz, tgt_len, embed_dim)\n",
        "\n",
        "        outputs = self.longformer_self_attn(\n",
        "            query,\n",
        "            attention_mask=attn_mask,\n",
        "            head_mask=None,\n",
        "            encoder_hidden_states=None,\n",
        "            encoder_attention_mask=None,\n",
        "        )\n",
        "\n",
        "        attn_output = outputs[0] \n",
        "        attn_output = attn_output.contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn_output = self.output(attn_output)\n",
        "\n",
        "        return (attn_output,) + outputs[1:] if len(outputs) == 2 else (attn_output, None)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn7I0nMPpyV6"
      },
      "source": [
        "# **Test model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYOQarg7rFx_"
      },
      "source": [
        "from transformers import BartTokenizer\n",
        "import torch\n",
        "model_path = \"/content/drive/MyDrive/longformers/models/long-bart-large-4096\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ZkbQ0wlCAn"
      },
      "source": [
        "model = LongBartForConditionalGeneration.from_pretrained(model_path)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Hz3uaSq4qb"
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(model_path)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnuoI3LallLC",
        "outputId": "253b7987-32fb-4eeb-8137-6d98f17e6542"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "padding = \"max_length\" \n",
        "\n",
        "text=\"\"\"On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View's securities as well as the securities of another microcap issuer, West Coast Ventures Group Corp.  The SEC further announced the institution of an order suspending trading in the securities of HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.In a complaint filed in the U.S. District Court for the Eastern District of New York, the SEC alleges that Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for HD View's common stock.  The scheme involved an undercover FBI agent who described his business as manipulating U.S. stocks through pump-and-dump schemes.  Kyriacou and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit.The SEC's complaint against Beaufort and Kyriacou alleges that they:opened brokerage accounts for the undercover agent in the names of nominees in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the undercover agent could create the false appearance that HD View's stock was liquid in advance of a pump-and-dump by \"gam[ing] the market\" through matched tradesexecuted multiple purchase orders of HD View shares with the understanding that Beaufort's client had arranged for an associate to simultaneously offer an equivalent number of shares at the same priceA second complaint filed by the SEC in the U.S. District Court for the Eastern District of New York alleges that in a series of recorded telephone conversations with the undercover agent, HD View CEO Dennis Mancino and William T. Hirschy agreed to manipulate HD View's common stock by using the agent's network of brokers to generate fraudulent retail demand for the stock in exchange for a kickback from the trading proceeds.  According to the complaint, the three men agreed that Mancino and Hirschy would manipulate HD View stock to a higher price before using the agent's brokers to liquidate their positions at an artificially inflated price.  The SEC's complaint also alleges that Mancino and Hirschy executed a \"test trade\" on Jan. 31, 2018, coordinated by the agent, consisting of a sell order placed by the defendants filled by an opposing purchase order placed by a broker into an account at Beaufort.  Unbeknownst to Mancino and Hirschy, the Beaufort account used for this trade was a nominal account that was opened and funded by the agent.  The SEC's complaint also alleges that, prior to their contact with the undercover agent, Mancino and Hirschy manipulated the market for HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with –including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC – to effect manipulative \"matched trades.\"The SEC's complaint against Beaufort and Kyriacou charges the defendants with violating Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder.  The SEC also charged Hirschy, Mancino, and their corporate entities with violating Section 17(a)(1) of the Securities Act of 1933, Sections 9(a)(1), 9(a)(2), and 10(b) of the Exchange Act and Rules 10b-5(a) and (c) thereunder.  The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, and penny stock bars from Beaufort and Kyriacou.  With respect to Hirschy, Mancino, and their corporate entities, the SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, penny stock bars, and an officer-and-director bar against Mancino.The investigation was conducted in the SEC's New York Regional Office by Tejal Shah and Joseph Darragh, Lorraine Collazo, and Michael D. Paley of the Microcap Fraud Task Force and supervised by Lara S. Mehraban, and in Washington, D.C. by Patrick L. Feeney, Robert Nesbitt, and Kevin Guerrero, and supervised by Antonia Chion.  Preethi Krishnamurthy and Ms. Shah will lead the SEC's litigation against Beaufort and Kyriacou.  Ann H. Petalas and Mr. Feeney, under the supervision of Cheryl Crumpton, will handle the SEC's litigation against Mancino, Hirschy, and their entities.  The SEC appreciates the assistance of the Office of the United States Attorney for the Eastern District of New York, the Federal Bureau of Investigation, the Internal Revenue Service, the Alberta Securities Commission, the Ontario Securities Commission, the Financial Conduct Authority of the United Kingdom, and the Financial Industry Regulatory Authority.The Commission's investigation in this matter is continuing.\"\"\"\n",
        "input_tokenized = tokenizer.encode(text, return_tensors='pt', max_length=4096).to(device)\n",
        "\n",
        "dim = list(input_tokenized.size())\n",
        "print(\"Input Shape (before padding to max_length): {}\".format(dim))\n",
        "\n",
        "\n",
        "input_tokenized = tokenizer.encode(text, return_tensors='pt',padding=padding,pad_to_max_length=True, padding_side='right', max_length=4096,truncation=True).to(device)\n",
        "\n",
        "dim = list(input_tokenized.size())\n",
        "print(\"Input Shape: {} (padded to max_length)\".format(dim))\n",
        "\n",
        "summary_ids = model.generate(input_tokenized,\n",
        "                                  num_beams=4,\n",
        "                                  no_repeat_ngram_size=3,\n",
        "                                  length_penalty=1.2,\n",
        "                                  min_length=350,\n",
        "                                  max_length=500)\n",
        "\n",
        "dim = list(summary_ids.size())\n",
        "print(\"Summary Shape: \", dim)\n",
        "  \n",
        "summ = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
        "\n",
        "print(\"Text: \")\n",
        "print(text)\n",
        "print(\"Summary: \")\n",
        "print(summ)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shape (before padding to max_length): [1, 1124]\n",
            "Input Shape: [1, 4096] (padded to max_length)\n",
            "Summary Shape:  [1, 499]\n",
            "Text: \n",
            "On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View's securities as well as the securities of another microcap issuer, West Coast Ventures Group Corp.  The SEC further announced the institution of an order suspending trading in the securities of HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.In a complaint filed in the U.S. District Court for the Eastern District of New York, the SEC alleges that Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for HD View's common stock.  The scheme involved an undercover FBI agent who described his business as manipulating U.S. stocks through pump-and-dump schemes.  Kyriacou and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit.The SEC's complaint against Beaufort and Kyriacou alleges that they:opened brokerage accounts for the undercover agent in the names of nominees in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the undercover agent could create the false appearance that HD View's stock was liquid in advance of a pump-and-dump by \"gam[ing] the market\" through matched tradesexecuted multiple purchase orders of HD View shares with the understanding that Beaufort's client had arranged for an associate to simultaneously offer an equivalent number of shares at the same priceA second complaint filed by the SEC in the U.S. District Court for the Eastern District of New York alleges that in a series of recorded telephone conversations with the undercover agent, HD View CEO Dennis Mancino and William T. Hirschy agreed to manipulate HD View's common stock by using the agent's network of brokers to generate fraudulent retail demand for the stock in exchange for a kickback from the trading proceeds.  According to the complaint, the three men agreed that Mancino and Hirschy would manipulate HD View stock to a higher price before using the agent's brokers to liquidate their positions at an artificially inflated price.  The SEC's complaint also alleges that Mancino and Hirschy executed a \"test trade\" on Jan. 31, 2018, coordinated by the agent, consisting of a sell order placed by the defendants filled by an opposing purchase order placed by a broker into an account at Beaufort.  Unbeknownst to Mancino and Hirschy, the Beaufort account used for this trade was a nominal account that was opened and funded by the agent.  The SEC's complaint also alleges that, prior to their contact with the undercover agent, Mancino and Hirschy manipulated the market for HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with –including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC – to effect manipulative \"matched trades.\"The SEC's complaint against Beaufort and Kyriacou charges the defendants with violating Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder.  The SEC also charged Hirschy, Mancino, and their corporate entities with violating Section 17(a)(1) of the Securities Act of 1933, Sections 9(a)(1), 9(a)(2), and 10(b) of the Exchange Act and Rules 10b-5(a) and (c) thereunder.  The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, and penny stock bars from Beaufort and Kyriacou.  With respect to Hirschy, Mancino, and their corporate entities, the SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, penny stock bars, and an officer-and-director bar against Mancino.The investigation was conducted in the SEC's New York Regional Office by Tejal Shah and Joseph Darragh, Lorraine Collazo, and Michael D. Paley of the Microcap Fraud Task Force and supervised by Lara S. Mehraban, and in Washington, D.C. by Patrick L. Feeney, Robert Nesbitt, and Kevin Guerrero, and supervised by Antonia Chion.  Preethi Krishnamurthy and Ms. Shah will lead the SEC's litigation against Beaufort and Kyriacou.  Ann H. Petalas and Mr. Feeney, under the supervision of Cheryl Crumpton, will handle the SEC's litigation against Mancino, Hirschy, and their entities.  The SEC appreciates the assistance of the Office of the United States Attorney for the Eastern District of New York, the Federal Bureau of Investigation, the Internal Revenue Service, the Alberta Securities Commission, the Ontario Securities Commission, the Financial Conduct Authority of the United Kingdom, and the Financial Industry Regulatory Authority.The Commission's investigation in this matter is continuing.\n",
            "Summary: \n",
            "These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York. On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.,S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating MHV's stock in the Securities Commission, and the Financial Industry Regulatory Authority's Fraud.The Commission's investigation into the manipulation of MHV was conducted in this matter is continuing. The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, the forfeiture of the stock bars from HD View, and their corporate entities, SEC's enforcement actions against MHV and their entities.In a complaint filed in the U.S. District Court for the District of Columbia, the SEC alleges that in the course of its investigation into MHV, the Financial Regulatory Authority, the Federal Deposit Insurance Corp., the Securities Exchange Commission, the United Kingdom, and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for MHV. The scheme involved an undercover FBI agent who described his business as manipulating U. S. stocks through pump-and-dump schemes.  He said that under the supervision of Cheryl and the SEC, he would lead the investigation against Mancino, Hirschy, and New York, and that the SEC would assist in the investigation of the Federal States Attorney, Mr. Robert, and Mr. Gerard.  Mr. Kevan, and Ms. Gerard, and Lora, and Lorasas. Mr. Gerard, and Mrs. Loras, and Robert. Mrion, and Dr. Robert.  Robert, Dr. John, and H.H. Hirschey.  H.R.Hirschey, and M. Hirschen.  Khirsh.  Ker.  Kar.  Kirschen, and K. K. Kirsch.  K.K. Kirch.  Kerry.  Krish.  Har.  Th. Th. Th. Thir. thir.Thir. Thyr.  thir.thirth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
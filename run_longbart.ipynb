{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run-longbart.ipynb",
      "provenance": [],
      "mount_file_id": "1JJ6VKjDcSfUmfVaF2MVraklZY82RF_T2",
      "authorship_tag": "ABX9TyMgnuNFoB828Iyy8uzG7Obr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsi319/LongFormer-Models/blob/main/run_longbart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arTxdl05mvtK",
        "outputId": "b28187b2-0ce3-4c7a-d1de-61984778ad80"
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "if path.exists(\"/content/drive/MyDrive/longformers/longbart\")==True:\n",
        "  %rm -r /content/drive/MyDrive/longformers/longbart\n",
        "  print(\"Deleted old repo\")\n",
        "!git clone https://github.com/nsi319/LongFormer-Models.git /content/drive/MyDrive/longformers/longbart \n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/drive/MyDrive/longformers/longbart'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 17 (delta 5), reused 13 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xDWbkzunVq9",
        "outputId": "ed5f5f03-f226-4382-bdde-2d903c57674d"
      },
      "source": [
        "!pip install transformers==2.11.0 "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
            "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
            "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
            "Collecting transformers==2.11.0\n",
            "  Using cached transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.10.0\n",
            "    Uninstalling transformers-2.10.0:\n",
            "      Successfully uninstalled transformers-2.10.0\n",
            "Successfully installed transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1A3pUaQpV8-",
        "outputId": "785ca6a1-5c35-4315-d339-e643b8cf06aa"
      },
      "source": [
        "!python /content/drive/MyDrive/longformers/longbart/bart-longformer/bart_to_longbart.py --help"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-12 19:13:45.121763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: bart_to_longbart.py [-h] --save_model_path SAVE_MODEL_PATH\n",
            "                           [--base_model BASE_MODEL]\n",
            "                           [--tokenizer_name_or_path TOKENIZER_NAME_OR_PATH]\n",
            "                           [--max_length MAX_LENGTH]\n",
            "                           [--attention_window ATTENTION_WINDOW]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --save_model_path SAVE_MODEL_PATH\n",
            "                        The path to save the converted model\n",
            "  --base_model BASE_MODEL\n",
            "                        The name or path of the base model you want to convert\n",
            "  --tokenizer_name_or_path TOKENIZER_NAME_OR_PATH\n",
            "                        The name or path of the tokenizer\n",
            "  --max_length MAX_LENGTH\n",
            "                        Maximum encoder positions\n",
            "  --attention_window ATTENTION_WINDOW\n",
            "                        Attention window size for longformer self attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L3il3Y4nIpK",
        "outputId": "64a17ff2-dc45-491f-c6a8-2c52a53c1ac8"
      },
      "source": [
        "!python /content/drive/MyDrive/longformers/longbart/bart-longformer/bart_to_longbart.py \\\n",
        "    --base_model facebook/bart-large \\\n",
        "    --tokenizer_name_or_path facebook/bart-large \\\n",
        "    --save_model_path /content/drive/MyDrive/longformers/models/long-bart-large-4096 \\\n",
        "    --attention_window 512 \\\n",
        "    --max_length 4096 \n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-12 19:14:59.323424: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "ModelArguments(save_model_path='/content/drive/MyDrive/longformers/models/long-bart-large-4096', base_model='facebook/bart-large', tokenizer_name_or_path='facebook/bart-large', max_length=4096, attention_window=512)\n",
            "<class '__main__.ModelArguments'>\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json from cache at /root/.cache/torch/transformers/7f6632e580b7d9fd4f611dd96dab877cccfc319867b53b8b72fddca7fd64de5c.8b65d3b9a47e96c1909d807f7e7f41dd1ed95092b139965be7b914aa4fb5fd08\n",
            "INFO:transformers.configuration_utils:Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_max_position_embeddings\": 1024,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"encoder_max_position_embeddings\": 1024,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin from cache at /root/.cache/torch/transformers/2e7cae41bb1dd1f18e498ff4ff0ea85f7e9bc2b637439e2d95c485c5d5bdd579.5442efe9b99297b6484ff9378cc5af7f47a37d305cbec072c44eb329873e4fe6\n",
            "INFO:transformers.modeling_utils:Weights of BartForConditionalGeneration not initialized from pretrained model: ['final_logits_bias']\n",
            "INFO:filelock:Lock 140444913993208 acquired on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpcq7kaqrj\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.40MB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json in cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:filelock:Lock 140444913993208 released on /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "INFO:filelock:Lock 140444913993208 acquired on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmps3qy6_op\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.82MB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt in cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:filelock:Lock 140444913993208 released on /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:__main__:saving model to /content/drive/MyDrive/longformers/models/long-bart-large-4096\n",
            "INFO:transformers.configuration_utils:Configuration saved in /content/drive/MyDrive/longformers/models/long-bart-large-4096/config.json\n",
            "INFO:transformers.modeling_utils:Model weights saved in /content/drive/MyDrive/longformers/models/long-bart-large-4096/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vlqECyDEdOf",
        "outputId": "3bda032f-9b2e-4f74-c6c4-9cdfde2b7ed6"
      },
      "source": [
        "!python /content/drive/MyDrive/longformers/longbart/bart-longformer/test_longbart.py \\\n",
        "    --model_path /content/drive/MyDrive/longformers/models/long-bart-large-4096"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-12 19:16:06.295546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Input Shape: [1, 4096] (padded to max_length)\n",
            "Summary Shape:  [1, 514]\n",
            "Text: \n",
            "The Securities and Exchange Commission charged William \"Bill\" Bowser, Christopher Ashby, Scott Beynon, and Jordan Nelson with securities fraud for misappropriating investor funds meant for the development and construction of new event centers. The SEC's complaint alleges that from approximately January 2017 to February 2019, Ashby, Beynon, and Nelson, through entities they controlled, sold investors interests in for-profit event centers purportedly being developed by Noah Corporation, an entity Bowser controlled. As alleged, Bowser diverted investor funds earmarked for specific properties and instead used them for Noah Corporation's and Bowser's operational and other expenses and to pay prior investors, rather than for construction of event centers as represented to investors. The complaint further alleges that contrary to their representations to investors, Ashby, Beynon, and Nelson failed to escrow investor funds and disbursed them to an entity Bowser controlled without having any controls in place to ensure that the disbursements were for legitimate expenses. The SEC's complaint, filed in federal district court in Utah, charges Bowser with violating the antifraud provisions of Sections 17(a)(1) and 17(a)(3) of the Securities Act of 1933 and Section 10(b) of the Securities Exchange Act of 1934 and Rules 10b-5(a) and (c) thereunder. The complaint charges Ashby, Beynon, and Nelson with violating the antifraud provisions of Sections 17(a)(2) and 17(a)(3) of the Securities Act and the registration provisions of Section 15(a) of the Exchange Act. Without admitting or denying the allegations of the complaint, the defendants have consented to judgments enjoining them from violating the charged provisions, ordering Bowser to pay disgorgement of $47,796 with prejudgment interest of $6,402 and a $192,768 penalty, ordering Ashby to pay disgorgement of $551,161 with prejudgment interest of $43,994 and a $96,384 penalty, ordering Beynon to pay disgorgement of $585,426 with prejudgment interest of $46,729 and a $96,384 penalty, and ordering Nelson to pay disgorgement of $281,273 with prejudgment interest of $22,451 and a $96,384 penalty. The SEC's investigation was conducted by Cheryl Mori and was supervised by Daniel Wadley and Amy Oliver of the Salt Lake Regional Office. The litigation will be led by Casey Fronk. The Securities and Exchange Commission today announced charges against DeAndre P. Sears and MASears LLC d/b/a Picasso Group, an entity he controlled and operated, with registration violations for unlawfully selling securities of Florida-based real estate firm EquiAlt LLC to retail investors. The SEC previously filed an enforcement action against EquiAlt LLC, its CEO Brian Davison, and its Managing Director Barry Rybicki on February 11, 2020, in connection with the alleged scheme. According to the SEC's complaint, between 2014 and 2020, Sears directly and indirectly, through the use of third-party agents, sold at least $25 million of EquiAlt's securities to more than 145 largely unaccredited, unsophisticated, and elderly retail investors located in 25 states. During that period, Sears was identified in EquiAlt private placement memoranda as Managing Director of Investments, President of Business Development and Marketing, or Vice President of Investor Relations. Sears, through Picasso Group, received approximately $3.5 million in transaction-based sales commissions from EquiAlt, despite neither being registered as broker dealers. The complaint alleges that beginning in approximately 2016, EquiAlt was actually operating a Ponzi scheme during which it raised more than $170 million from approximately 1,100 investors in 35 states. The SEC's complaint charges Sears and Picasso Group with violating the securities registration provisions of Sections 5(a) and 5(c) of the Securities Act of 1933, and the broker-dealer registration provisions of Section 15(a)(1) of the Securities Exchange Act of 1934. Without admitting or denying the allegations in the complaint, Sears and Picasso Group have agreed to the entry of a judgment providing injunctive relief with disgorgement and civil penalties to be determined by a court at a later date. Sears also agreed to associational and penny stock bars as part of a settled follow-on administrative proceeding. The SEC's continuing investigation is being conducted by Chanel T. Rowe and Andre Zamorano, with assistance from Mark Dee, and supervised by Thierry Olivier Desmet and Glenn S. Gordon in the Miami Regional Office. The SEC's litigation is being led by Alise Johnson and supervised by Andrew O. Schiff. The SEC encourages investors to check the backgrounds of people selling investments by using the SEC's Investor.gov to identify quickly whether they are registered professionals and confirm their identity. On December 30, 2020, the U. S. District Court for the Southern District of New York entered final judgments against two penny stock promoters whom the Commission charged in connection with several alleged pump-and-dump schemes involving stocks they were touting in their supposedly independent newsletters. The SEC's complaint in this action, filed in November 2014, alleged that Anthony Thompson, Jr., Jay Fung, and a third defendant, Eric Van Nguyen, worked in concert to gain control of a large portion of shares in the stock of microcap companies, then hyped those stocks in newsletters they distributed to prospective investors. According to the complaint, the newsletters published by Thompson, Fung, and Van Nguyen misleadingly stated that they \"may\" or \"might\" sell shares they owned when in reality they always intended to sell -- and in some instances already were selling - the stocks they were promoting. As alleged, they also failed to fully disclose in their newsletters the amounts of compensation they were receiving for promoting the stocks. Thompson and Fung, who were both previously convicted in a parallel state criminal case for conduct that was the subject of the SEC's action, each consented to the entry of a final judgment enjoining them from future violations of the anti-touting provisions of Section 17(b) of the Securities Act of 1933. Thompson further consented to pay disgorgement of $624,882 plus prejudgment interest of $137,381, and Fung consented to pay disgorgement of $1,766,083 plus prejudgment interest of $244,308. Fung's obligation to pay disgorgement and prejudgment interest will be deemed satisfied by the $2,800,000 he was ordered to pay pursuant to an order of restitution entered in a related criminal action filed by the Manhattan District Attorney's Office. The SEC's litigation continues with respect to Van Nguyen. The SEC's litigation has been handled by Peter Pizzani, Mark R. Sylvester, and Thomas P. Smith, Jr., and supervised by Sanjay Wadhwa, all of the New York Regional Office.\n",
            "Summary: \n",
            "The Securities and Exchange Commission (SEC) has announced that it has filed a civil case for conduct that was the subject of the SEC's action, each consented to the entry of a final judgment of $2,800. The SEC's complaint in this action, filed in November 2014, alleged that Anthony Thompson, Jr., Jay Fung, and a third defendant, Eric Van Nguyen, worked in concert to gain control of a large portion of shares in the stock of microcap companies, then hyped those stocks in newsletters they distributed to prospective investors. As alleged, they also failed to fully disclose in their newsletters the amounts of compensation they were receiving for promoting the stocks. According to the complaint, the newsletters published by Thompson, Fung and Van Nguyen misleadingly stated that they \"may\" or \"might\" sell shares they owned when in reality they always intended to sell -- and in some instances already were selling - the stocks they were promoting. Thompson and Fung have been charged with securities fraud and conspiracy to commit securities fraud, and they have been ordered to pay a total of $1,766,308.00 in the amount of the Securities and Derivatives Securities Act of 1933. Thompson's attorney, Michael J. Fung's, declined to comment on the allegations. SEC's litigation continues with respect to the settlement of the case against Thompson, Van Nguyen and the other defendants in the case, which is being conducted by the Securities & Exchange Commission's Office. The complaint charges Ashby, Beynon, and Nelson with violating the antifraud provisions of Sections 17(a)(2) and 17(c) of the Exchange Act and the registration provisions of Section 15(a) of Securities Exchange Act of 1934. SEC and the Office of the Inspector General of the United States of America have agreed to settle the case for $624,000. The settlement will be paid by the SEC and will be deemed satisfied by the New York State Attorney General's office. SEC has also agreed to pay disgorgano disgorgnet interest of $3,500.00 to each of the defendants. Investor's interest in the settlement is being paid in the form of $5,000,000 in cash.00.00, plus interest, and $5.00 per share. The securities are being consented by the three defendants.SOURCE: SEC.gov, www.sec.gov/investigations/securities-and-derivatives-statutes-of-us-a-jurisdiction\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "led-longbart-finetune",
      "provenance": [],
      "collapsed_sections": [
        "XjuNWr-CRwB-",
        "6lJrlWSzRno-",
        "R7zoZ4CFqBwm",
        "2R67ClV7pg20",
        "d-fauWiVptTV",
        "nn7I0nMPpyV6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75a55ed2b2db49eb95710af4dace04a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14ab11f4a97842d49b7dab26041dbaad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8fb73afb226b41d39783d3a1f28b40fa",
              "IPY_MODEL_f261b036e96c45888d95d8dd0d52ce98"
            ]
          }
        },
        "14ab11f4a97842d49b7dab26041dbaad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fb73afb226b41d39783d3a1f28b40fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02b2589951a44144a4d20f417f56ff64",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1092,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1092,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dc7aee21c68486e88b5091f8cbbe360"
          }
        },
        "f261b036e96c45888d95d8dd0d52ce98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1efc9eff64e4ce8a90004d30d0c9150",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.09k/1.09k [00:20&lt;00:00, 52.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83337e755428417c8b3dede58300feea"
          }
        },
        "02b2589951a44144a4d20f417f56ff64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dc7aee21c68486e88b5091f8cbbe360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1efc9eff64e4ce8a90004d30d0c9150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83337e755428417c8b3dede58300feea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc568476e93f4328832522b20294888d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b2316f21cd04576ab52147f369c8931",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e8bc2eccc3554e0bb4842f1af6d6b330",
              "IPY_MODEL_b5bea2125cfe42238373447bf4bca7b0"
            ]
          }
        },
        "4b2316f21cd04576ab52147f369c8931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8bc2eccc3554e0bb4842f1af6d6b330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63718a5d177d42caa7b6959bb0c665bb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 647693783,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 647693783,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bddb93fcead14b688c2f9cd1572b0f80"
          }
        },
        "b5bea2125cfe42238373447bf4bca7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1d797e2f26143a2b7a2f9e72386d1f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 648M/648M [00:10&lt;00:00, 60.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8ad1b3b9ea6457987e701ee4bbaa2d0"
          }
        },
        "63718a5d177d42caa7b6959bb0c665bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bddb93fcead14b688c2f9cd1572b0f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1d797e2f26143a2b7a2f9e72386d1f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8ad1b3b9ea6457987e701ee4bbaa2d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c891910e8c67463792116106845e665b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dcd65ac1edf64b9a8316eca14a338d21",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3369f4f173f34540be96161772361a5e",
              "IPY_MODEL_88f2677173514cab93649cd4e29a07f9"
            ]
          }
        },
        "dcd65ac1edf64b9a8316eca14a338d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3369f4f173f34540be96161772361a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_720b505f496e4bfb992c34ddbd9a3a77",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b22d39bbd729420baf4918a766a5234d"
          }
        },
        "88f2677173514cab93649cd4e29a07f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b9b2c917404d43669e227888cb369e5b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.38MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2cdefa8f20554ce0ad270b3e648b870f"
          }
        },
        "720b505f496e4bfb992c34ddbd9a3a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b22d39bbd729420baf4918a766a5234d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9b2c917404d43669e227888cb369e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2cdefa8f20554ce0ad270b3e648b870f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "646ba6c6781b43a78306ec2e943dc1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2ba9dd7f9b124c27838fed284fc8478a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61912ad458d24425a4bdeb4fcf73ec4f",
              "IPY_MODEL_c20cba3a47b340dea0666fb29d6dccce"
            ]
          }
        },
        "2ba9dd7f9b124c27838fed284fc8478a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61912ad458d24425a4bdeb4fcf73ec4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8f0d887125140d292e06fe834389adc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c5d6daf31f54b3e85411254945abad8"
          }
        },
        "c20cba3a47b340dea0666fb29d6dccce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03d0710514244e54af3204351a0a1e09",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 1.48MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e96dd7b470a4f8591508c421bf7592b"
          }
        },
        "c8f0d887125140d292e06fe834389adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c5d6daf31f54b3e85411254945abad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03d0710514244e54af3204351a0a1e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e96dd7b470a4f8591508c421bf7592b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWMc09rgrmUU"
      },
      "source": [
        "!git clone https://github.com/nsi319/Finetune-Transformers.git /content/drive/MyDrive/Finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xDWbkzunVq9",
        "outputId": "b62fc1b4-5190-476a-dd4e-31634d69c719"
      },
      "source": [
        "!pip install transformers==4.2.0\n",
        "!pip install rouge_score\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.2.0 in /usr/local/lib/python3.6/dist-packages (4.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.2.0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.0) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (2.10)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score) (0.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.19.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.6/dist-packages (from datasets) (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6klFPOTGWdUB"
      },
      "source": [
        "**PREPARE TRAINING AND VALIDATION DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb5Em1pFslBZ",
        "outputId": "60ec4ce6-0dd8-4af8-ac8d-692b13851835"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/summarize/legal-summarization/2018_clean.csv')\n",
        "df = df[[\"Text\", \"Summary\"]]\n",
        "df[df.columns] = df.apply(lambda x: x.str.strip())\n",
        "\n",
        "df[\"Summary\"] = df[\"Summary\"].apply(lambda x: \"\".join(x.splitlines()))\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x: \"\".join(x.splitlines()))\n",
        "\n",
        "length = [ [len(row[\"Summary\"].split(\" \")), len(row[\"Text\"].split(\" \"))]  for index,row in df.iterrows()]\n",
        "\n",
        "mean_text_length = int(np.mean(list(list(zip(*length))[1])))\n",
        "mean_summ_length = int(np.mean(list(list(zip(*length))[0])))\n",
        "\n",
        "max_text_length = int(np.max(list(list(zip(*length))[1])))\n",
        "max_summ_length = int(np.max(list(list(zip(*length))[0])))\n",
        "\n",
        "print(\"Max Text Length: \",max_text_length)\n",
        "print(\"Max Summary Length: \",max_summ_length)\n",
        "\n",
        "      \n",
        "print(\"Mean Text Length: \",mean_text_length)\n",
        "print(\"Mean Summary Length: \",mean_summ_length)\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.15)\n",
        "\n",
        "df_train.to_csv(\"/content/drive/MyDrive/longformers/data/train.csv\")\n",
        "print(len(df_train))\n",
        "\n",
        "df_test.to_csv(\"/content/drive/MyDrive/longformers/data/valid.csv\")\n",
        "print(len(df_test))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Text Length:  6747\n",
            "Max Summary Length:  816\n",
            "Mean Text Length:  3539\n",
            "Mean Summary Length:  338\n",
            "79\n",
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J42W-9mGUXAk",
        "outputId": "7307c3f6-b0a8-42fd-cbe5-6797c5de20e8"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=0\n",
        "%env CUDA_LAUNCH_BLOCKING=0\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOR_4L-DeS31",
        "outputId": "9ab34145-a25d-4184-d0ee-86d197be7adc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 16 16:07:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzJGSVJjWIF9"
      },
      "source": [
        "# **Finetune allenai/led-base-16384**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Nj4e85WNCM",
        "outputId": "b2a3001c-4001-458a-8872-29f400e07ab9"
      },
      "source": [
        "!python /content/drive/MyDrive/Finetune/run.py  \\\n",
        "    --model_name_or_path allenai/led-base-16384 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --task summarization \\\n",
        "    --max_source_length=4096 \\\n",
        "    --max_target_length=550 \\\n",
        "    --num_beams=3 \\\n",
        "    --min_summ_length=350 \\\n",
        "    --max_summ_length=500 \\\n",
        "    --length_penalty=2.0 \\\n",
        "    --train_file /content/drive/MyDrive/longformers/data/train.csv \\\n",
        "    --validation_file /content/drive/MyDrive/longformers/data/valid.csv \\\n",
        "    --output_dir /content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split\\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --predict_with_generate \\\n",
        "    --text_column Text \\\n",
        "    --summary_column Summary "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-16 16:08:54.193325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/16/2021 16:08:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/16/2021 16:08:55 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Feb16_16-08-55_52a0b847f5ba', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], sortish_sampler=False, predict_with_generate=True)\n",
            "02/16/2021 16:08:56 - WARNING - datasets.builder -   Using custom data configuration default-6d39b139f526611d\n",
            "02/16/2021 16:08:56 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-6d39b139f526611d/0.0.0/965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93)\n",
            "loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7\n",
            "Model config LEDConfig {\n",
            "  \"_name_or_path\": \"./\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"LEDForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_window\": [\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_decoder_position_embeddings\": 1024,\n",
            "  \"max_encoder_position_embeddings\": 16384,\n",
            "  \"model_type\": \"led\",\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.3.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7\n",
            "Model config LEDConfig {\n",
            "  \"_name_or_path\": \"./\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"LEDForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_window\": [\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_decoder_position_embeddings\": 1024,\n",
            "  \"max_encoder_position_embeddings\": 16384,\n",
            "  \"model_type\": \"led\",\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.3.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/allenai/led-base-16384/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/4fb25bb1f9a942a2e2930029211b4a7deaeb18b62f6e5ce6d59730c90da51373.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "loading file https://huggingface.co/allenai/led-base-16384/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/087e8f4306cbf22e21907929074344a3b0a46bd680a118eb6267cd5a2bcec5b2.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer.json from cache at None\n",
            "loading weights file https://huggingface.co/allenai/led-base-16384/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c8f7e4603efbc329ce921b34057d78880dead50f45b2a1648b3a06ca6eb17f51.201222b06d46289037a8dccc57548abc8eb81ba042d3762214ac15c9691ff8c7\n",
            "All model checkpoint weights were used when initializing LEDForConditionalGeneration.\n",
            "\n",
            "All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00,  1.19ba/s]\n",
            "100% 1/1 [00:00<00:00,  7.11ba/s]\n",
            "The following columns in the training set don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: .\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: .\n",
            "***** Running training *****\n",
            "  Num examples = 79\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 237\n",
            "  0% 0/237 [00:00<?, ?it/s]Input ids are automatically padded from 3599 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  0% 1/237 [00:01<06:13,  1.58s/it]Input ids are automatically padded from 3582 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  2% 5/237 [00:07<05:43,  1.48s/it]Input ids are automatically padded from 3554 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  3% 6/237 [00:08<05:36,  1.46s/it]Input ids are automatically padded from 3542 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  3% 7/237 [00:10<05:32,  1.45s/it]Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            "  4% 9/237 [00:12<05:08,  1.35s/it]Input ids are automatically padded from 3113 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  5% 11/237 [00:15<05:17,  1.40s/it]Input ids are automatically padded from 3533 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  6% 15/237 [00:21<05:17,  1.43s/it]Input ids are automatically padded from 2988 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            "  7% 16/237 [00:22<04:56,  1.34s/it]Input ids are automatically padded from 3109 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  8% 19/237 [00:26<05:10,  1.42s/it]Input ids are automatically padded from 3673 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "  8% 20/237 [00:28<05:10,  1.43s/it]Input ids are automatically padded from 1889 to 2048 to be a multiple of `config.attention_window`: 1024\n",
            "  9% 22/237 [00:30<04:37,  1.29s/it]Input ids are automatically padded from 3528 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 11% 25/237 [00:34<04:57,  1.40s/it]Input ids are automatically padded from 3641 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 12% 28/237 [00:39<05:07,  1.47s/it]Input ids are automatically padded from 3403 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 14% 33/237 [00:46<05:02,  1.48s/it]Input ids are automatically padded from 3843 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 16% 39/237 [00:55<04:55,  1.49s/it]Input ids are automatically padded from 3701 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 17% 40/237 [00:57<04:53,  1.49s/it]Input ids are automatically padded from 2619 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 19% 45/237 [01:04<04:41,  1.47s/it]Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 1024\n",
            " 20% 47/237 [01:06<03:55,  1.24s/it]Input ids are automatically padded from 2614 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 22% 51/237 [01:11<04:23,  1.42s/it]Input ids are automatically padded from 3787 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 24% 57/237 [01:20<04:31,  1.51s/it]Input ids are automatically padded from 3120 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 24% 58/237 [01:22<04:27,  1.49s/it]Input ids are automatically padded from 4063 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 28% 67/237 [01:36<04:17,  1.51s/it]Input ids are automatically padded from 4017 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 33% 78/237 [01:52<04:01,  1.52s/it]Input ids are automatically padded from 3475 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 34% 81/237 [01:57<03:51,  1.48s/it]Input ids are automatically padded from 3787 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 35% 84/237 [02:01<03:46,  1.48s/it]Input ids are automatically padded from 3120 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 36% 86/237 [02:04<03:44,  1.49s/it]Input ids are automatically padded from 2614 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 37% 87/237 [02:05<03:26,  1.38s/it]Input ids are automatically padded from 3641 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 39% 92/237 [02:13<03:37,  1.50s/it]Input ids are automatically padded from 3843 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 40% 94/237 [02:16<03:31,  1.48s/it]Input ids are automatically padded from 3475 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 40% 95/237 [02:17<03:30,  1.49s/it]Input ids are automatically padded from 3542 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 43% 101/237 [02:26<03:23,  1.50s/it]Input ids are automatically padded from 1889 to 2048 to be a multiple of `config.attention_window`: 1024\n",
            " 43% 103/237 [02:28<03:00,  1.35s/it]Input ids are automatically padded from 3109 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 45% 106/237 [02:33<03:10,  1.46s/it]Input ids are automatically padded from 4063 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 46% 109/237 [02:37<03:10,  1.49s/it]Input ids are automatically padded from 3673 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 49% 115/237 [02:46<03:02,  1.50s/it]Input ids are automatically padded from 3403 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 50% 118/237 [02:51<02:59,  1.51s/it]Input ids are automatically padded from 3701 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 52% 123/237 [02:58<02:50,  1.50s/it]Input ids are automatically padded from 3554 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 52% 124/237 [03:00<02:47,  1.48s/it]Input ids are automatically padded from 4017 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 53% 126/237 [03:03<02:47,  1.51s/it]Input ids are automatically padded from 3582 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 54% 129/237 [03:07<02:42,  1.50s/it]Input ids are automatically padded from 3599 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 55% 130/237 [03:09<02:39,  1.49s/it]Input ids are automatically padded from 2619 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 55% 131/237 [03:10<02:26,  1.38s/it]Input ids are automatically padded from 2988 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 61% 144/237 [03:29<02:20,  1.51s/it]Input ids are automatically padded from 3528 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 62% 146/237 [03:32<02:17,  1.51s/it]Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 64% 151/237 [03:40<02:09,  1.50s/it]Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 1024\n",
            " 65% 155/237 [03:44<01:53,  1.38s/it]Input ids are automatically padded from 3533 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 66% 156/237 [03:46<01:54,  1.41s/it]Input ids are automatically padded from 3113 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 67% 159/237 [03:50<01:53,  1.45s/it]Input ids are automatically padded from 3582 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 69% 163/237 [03:56<01:50,  1.49s/it]Input ids are automatically padded from 3787 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 69% 164/237 [03:58<01:48,  1.48s/it]Input ids are automatically padded from 3554 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 71% 168/237 [04:04<01:44,  1.51s/it]Input ids are automatically padded from 3599 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 73% 173/237 [04:11<01:35,  1.50s/it]Input ids are automatically padded from 2988 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 74% 175/237 [04:14<01:29,  1.44s/it]Input ids are automatically padded from 3109 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 78% 184/237 [04:28<01:18,  1.49s/it]Input ids are automatically padded from 3120 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 79% 187/237 [04:32<01:14,  1.49s/it]Input ids are automatically padded from 4063 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 82% 194/237 [04:43<01:04,  1.51s/it]Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 84% 199/237 [04:50<00:56,  1.48s/it]Input ids are automatically padded from 3673 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 85% 202/237 [04:54<00:51,  1.48s/it]Input ids are automatically padded from 2619 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 86% 203/237 [04:55<00:46,  1.38s/it]Input ids are automatically padded from 1889 to 2048 to be a multiple of `config.attention_window`: 1024\n",
            " 86% 204/237 [04:56<00:39,  1.20s/it]Input ids are automatically padded from 3533 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 86% 205/237 [04:58<00:40,  1.28s/it]Input ids are automatically padded from 3403 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 89% 210/237 [05:05<00:39,  1.47s/it]Input ids are automatically padded from 3113 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 89% 212/237 [05:08<00:37,  1.49s/it]Input ids are automatically padded from 3701 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 90% 213/237 [05:10<00:35,  1.49s/it]Input ids are automatically padded from 4017 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 92% 217/237 [05:16<00:30,  1.50s/it]Input ids are automatically padded from 3843 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 92% 218/237 [05:17<00:28,  1.50s/it]Input ids are automatically padded from 2614 to 3072 to be a multiple of `config.attention_window`: 1024\n",
            " 94% 222/237 [05:23<00:21,  1.46s/it]Input ids are automatically padded from 3641 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 96% 227/237 [05:30<00:14,  1.48s/it]Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 1024\n",
            " 96% 228/237 [05:31<00:10,  1.16s/it]Input ids are automatically padded from 3475 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 99% 234/237 [05:40<00:04,  1.46s/it]Input ids are automatically padded from 3542 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            " 99% 235/237 [05:41<00:02,  1.47s/it]Input ids are automatically padded from 3528 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "{'train_runtime': 344.5836, 'train_samples_per_second': 0.688, 'epoch': 3.0}\n",
            "100% 237/237 [05:44<00:00,  1.45s/it]\n",
            "Saving model checkpoint to /content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split\n",
            "Configuration saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split/config.json\n",
            "Model weights saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split/pytorch_model.bin\n",
            "02/16/2021 16:15:00 - INFO - __main__ -   ***** Train results *****\n",
            "02/16/2021 16:15:00 - INFO - __main__ -     epoch = 3.0\n",
            "02/16/2021 16:15:00 - INFO - __main__ -     train_runtime = 344.5836\n",
            "02/16/2021 16:15:00 - INFO - __main__ -     train_samples_per_second = 0.688\n",
            "\n",
            "\n",
            "Running Evaluation Script\n",
            "1it [00:04,  4.60s/it]Input ids are automatically padded from 3743 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "2it [00:09,  4.79s/it]Input ids are automatically padded from 3082 to 4096 to be a multiple of `config.attention_window`: 1024\n",
            "4it [00:20,  5.02s/it]Input ids are automatically padded from 1690 to 2048 to be a multiple of `config.attention_window`: 1024\n",
            "14it [01:02,  4.47s/it]\n",
            "Evaluation Completed\n",
            "Evaluation results saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split/14-test_results.csv\n",
            "Evaluation scores saved in /content/drive/MyDrive/longformers/models/legal-led-base-16384_79_14_split/evaluation_scores.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lJrlWSzRno-"
      },
      "source": [
        "# **Test led-base-16384 with large text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221,
          "referenced_widgets": [
            "75a55ed2b2db49eb95710af4dace04a4",
            "14ab11f4a97842d49b7dab26041dbaad",
            "8fb73afb226b41d39783d3a1f28b40fa",
            "f261b036e96c45888d95d8dd0d52ce98",
            "02b2589951a44144a4d20f417f56ff64",
            "4dc7aee21c68486e88b5091f8cbbe360",
            "b1efc9eff64e4ce8a90004d30d0c9150",
            "83337e755428417c8b3dede58300feea",
            "bc568476e93f4328832522b20294888d",
            "4b2316f21cd04576ab52147f369c8931",
            "e8bc2eccc3554e0bb4842f1af6d6b330",
            "b5bea2125cfe42238373447bf4bca7b0",
            "63718a5d177d42caa7b6959bb0c665bb",
            "bddb93fcead14b688c2f9cd1572b0f80",
            "c1d797e2f26143a2b7a2f9e72386d1f5",
            "a8ad1b3b9ea6457987e701ee4bbaa2d0",
            "c891910e8c67463792116106845e665b",
            "dcd65ac1edf64b9a8316eca14a338d21",
            "3369f4f173f34540be96161772361a5e",
            "88f2677173514cab93649cd4e29a07f9",
            "720b505f496e4bfb992c34ddbd9a3a77",
            "b22d39bbd729420baf4918a766a5234d",
            "b9b2c917404d43669e227888cb369e5b",
            "2cdefa8f20554ce0ad270b3e648b870f",
            "646ba6c6781b43a78306ec2e943dc1b3",
            "2ba9dd7f9b124c27838fed284fc8478a",
            "61912ad458d24425a4bdeb4fcf73ec4f",
            "c20cba3a47b340dea0666fb29d6dccce",
            "c8f0d887125140d292e06fe834389adc",
            "1c5d6daf31f54b3e85411254945abad8",
            "03d0710514244e54af3204351a0a1e09",
            "6e96dd7b470a4f8591508c421bf7592b"
          ]
        },
        "id": "9mHSLb7NOQ-a",
        "outputId": "56733d24-68e6-4732-8cff-6d6cd58c5f85"
      },
      "source": [
        "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
        "\n",
        "\n",
        "model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n",
        "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75a55ed2b2db49eb95710af4dace04a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1092.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc568476e93f4328832522b20294888d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=647693783.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c891910e8c67463792116106845e665b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "646ba6c6781b43a78306ec2e943dc1b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk_qYXHWO6tx",
        "outputId": "293d2887-f185-4222-a082-f9919a75a3b4"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "padding = \"max_length\" \n",
        "\n",
        "text=\"\"\"On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View's securities as well as the securities of another microcap issuer, West Coast Ventures Group Corp.  The SEC further announced the institution of an order suspending trading in the securities of HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.In a complaint filed in the U.S. District Court for the Eastern District of New York, the SEC alleges that Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for HD View's common stock.  The scheme involved an undercover FBI agent who described his business as manipulating U.S. stocks through pump-and-dump schemes.  Kyriacou and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit.The SEC's complaint against Beaufort and Kyriacou alleges that they:opened brokerage accounts for the undercover agent in the names of nominees in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the undercover agent could create the false appearance that HD View's stock was liquid in advance of a pump-and-dump by \"gam[ing] the market\" through matched tradesexecuted multiple purchase orders of HD View shares with the understanding that Beaufort's client had arranged for an associate to simultaneously offer an equivalent number of shares at the same priceA second complaint filed by the SEC in the U.S. District Court for the Eastern District of New York alleges that in a series of recorded telephone conversations with the undercover agent, HD View CEO Dennis Mancino and William T. Hirschy agreed to manipulate HD View's common stock by using the agent's network of brokers to generate fraudulent retail demand for the stock in exchange for a kickback from the trading proceeds.  According to the complaint, the three men agreed that Mancino and Hirschy would manipulate HD View stock to a higher price before using the agent's brokers to liquidate their positions at an artificially inflated price.  The SEC's complaint also alleges that Mancino and Hirschy executed a \"test trade\" on Jan. 31, 2018, coordinated by the agent, consisting of a sell order placed by the defendants filled by an opposing purchase order placed by a broker into an account at Beaufort.  Unbeknownst to Mancino and Hirschy, the Beaufort account used for this trade was a nominal account that was opened and funded by the agent.  The SEC's complaint also alleges that, prior to their contact with the undercover agent, Mancino and Hirschy manipulated the market for HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with â€“including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC â€“ to effect manipulative \"matched trades.\"The SEC's complaint against Beaufort and Kyriacou charges the defendants with violating Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder.  The SEC also charged Hirschy, Mancino, and their corporate entities with violating Section 17(a)(1) of the Securities Act of 1933, Sections 9(a)(1), 9(a)(2), and 10(b) of the Exchange Act and Rules 10b-5(a) and (c) thereunder.  The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, and penny stock bars from Beaufort and Kyriacou.  With respect to Hirschy, Mancino, and their corporate entities, the SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, penny stock bars, and an officer-and-director bar against Mancino.The investigation was conducted in the SEC's New York Regional Office by Tejal Shah and Joseph Darragh, Lorraine Collazo, and Michael D. Paley of the Microcap Fraud Task Force and supervised by Lara S. Mehraban, and in Washington, D.C. by Patrick L. Feeney, Robert Nesbitt, and Kevin Guerrero, and supervised by Antonia Chion.  Preethi Krishnamurthy and Ms. Shah will lead the SEC's litigation against Beaufort and Kyriacou.  Ann H. Petalas and Mr. Feeney, under the supervision of Cheryl Crumpton, will handle the SEC's litigation against Mancino, Hirschy, and their entities.  The SEC appreciates the assistance of the Office of the United States Attorney for the Eastern District of New York, the Federal Bureau of Investigation, the Internal Revenue Service, the Alberta Securities Commission, the Ontario Securities Commission, the Financial Conduct Authority of the United Kingdom, and the Financial Industry Regulatory Authority.The Commission's investigation in this matter is continuing.\"\"\"\n",
        "input_tokenized = tokenizer.encode(text, return_tensors='pt', max_length=409,truncation=True).to(device)\n",
        "\n",
        "dim = list(input_tokenized.size())\n",
        "print(\"Input Shape (before padding to max_length): {}\".format(dim))\n",
        "\n",
        "\n",
        "input_tokenized = tokenizer.encode(text, return_tensors='pt',padding=padding,pad_to_max_length=True, max_length=4096,truncation=True).to(device)\n",
        "\n",
        "dim = list(input_tokenized.size())\n",
        "print(\"Input Shape: {} (padded to max_length)\".format(dim))\n",
        "\n",
        "summary_ids = model.generate(input_tokenized,\n",
        "                                  num_beams=4,\n",
        "                                  no_repeat_ngram_size=3,\n",
        "                                  length_penalty=1.2,\n",
        "                                  min_length=350,\n",
        "                                  max_length=500)\n",
        "\n",
        "dim = list(summary_ids.size())\n",
        "print(\"Summary Shape: \", dim)\n",
        "  \n",
        "summ = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
        "\n",
        "print(\"Text: \")\n",
        "print(text)\n",
        "print(\"Summary: \")\n",
        "print(summ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shape (before padding to max_length): [1, 409]\n",
            "Input Shape: [1, 4096] (padded to max_length)\n",
            "Summary Shape:  [1, 500]\n",
            "Text: \n",
            "On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View's securities as well as the securities of another microcap issuer, West Coast Ventures Group Corp.  The SEC further announced the institution of an order suspending trading in the securities of HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.In a complaint filed in the U.S. District Court for the Eastern District of New York, the SEC alleges that Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for HD View's common stock.  The scheme involved an undercover FBI agent who described his business as manipulating U.S. stocks through pump-and-dump schemes.  Kyriacou and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit.The SEC's complaint against Beaufort and Kyriacou alleges that they:opened brokerage accounts for the undercover agent in the names of nominees in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the undercover agent could create the false appearance that HD View's stock was liquid in advance of a pump-and-dump by \"gam[ing] the market\" through matched tradesexecuted multiple purchase orders of HD View shares with the understanding that Beaufort's client had arranged for an associate to simultaneously offer an equivalent number of shares at the same priceA second complaint filed by the SEC in the U.S. District Court for the Eastern District of New York alleges that in a series of recorded telephone conversations with the undercover agent, HD View CEO Dennis Mancino and William T. Hirschy agreed to manipulate HD View's common stock by using the agent's network of brokers to generate fraudulent retail demand for the stock in exchange for a kickback from the trading proceeds.  According to the complaint, the three men agreed that Mancino and Hirschy would manipulate HD View stock to a higher price before using the agent's brokers to liquidate their positions at an artificially inflated price.  The SEC's complaint also alleges that Mancino and Hirschy executed a \"test trade\" on Jan. 31, 2018, coordinated by the agent, consisting of a sell order placed by the defendants filled by an opposing purchase order placed by a broker into an account at Beaufort.  Unbeknownst to Mancino and Hirschy, the Beaufort account used for this trade was a nominal account that was opened and funded by the agent.  The SEC's complaint also alleges that, prior to their contact with the undercover agent, Mancino and Hirschy manipulated the market for HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with â€“including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC â€“ to effect manipulative \"matched trades.\"The SEC's complaint against Beaufort and Kyriacou charges the defendants with violating Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder.  The SEC also charged Hirschy, Mancino, and their corporate entities with violating Section 17(a)(1) of the Securities Act of 1933, Sections 9(a)(1), 9(a)(2), and 10(b) of the Exchange Act and Rules 10b-5(a) and (c) thereunder.  The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, and penny stock bars from Beaufort and Kyriacou.  With respect to Hirschy, Mancino, and their corporate entities, the SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, penny stock bars, and an officer-and-director bar against Mancino.The investigation was conducted in the SEC's New York Regional Office by Tejal Shah and Joseph Darragh, Lorraine Collazo, and Michael D. Paley of the Microcap Fraud Task Force and supervised by Lara S. Mehraban, and in Washington, D.C. by Patrick L. Feeney, Robert Nesbitt, and Kevin Guerrero, and supervised by Antonia Chion.  Preethi Krishnamurthy and Ms. Shah will lead the SEC's litigation against Beaufort and Kyriacou.  Ann H. Petalas and Mr. Feeney, under the supervision of Cheryl Crumpton, will handle the SEC's litigation against Mancino, Hirschy, and their entities.  The SEC appreciates the assistance of the Office of the United States Attorney for the Eastern District of New York, the Federal Bureau of Investigation, the Internal Revenue Service, the Alberta Securities Commission, the Ontario Securities Commission, the Financial Conduct Authority of the United Kingdom, and the Financial Industry Regulatory Authority.The Commission's investigation in this matter is continuing.\n",
            "Summary: \n",
            "On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., an U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with â€“including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC â€“ to effect manipulative \"matched trades.\"The SEC further announced the institution of an order suspending trading in HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.The SEC filed a complaint against Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, with the SEC alleging that Beaufort was engaged in a scheme to manipulate the market for HD View, which occurred on Jan. 31, 2018. The SEC alleges that the scheme involved an undercover FBI agent who described his business as \"pumping-and-dump schemes.  He also described the scheme as being a \"pump and dump scheme.\" The SEC alleged that the undercover agent and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit. The scheme involved a number of actions, including the following:The SEC's complaint against the undercover FBI agents, including: Beaufortâ€™s agent, Beaufort's agent and his network of brokers in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the agent could create the false appearance that HD View stock was liquid in advance of a pump- and-dump by \"gam[ing] the market. The agent's network of brokerage accounts were opened and funded by the agent. The undercover agent was able to manipulate HD View shares with the understanding that the associate to simultaneously offer an equivalent number of shares at the same priceThe SEC also alleges that in a series of recorded telephone conversations with Mancino and Hirschy, the three men agreed to use the agent's brokers to generate fraudulent retail\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4ms-G5qMrp"
      },
      "source": [
        "#**LongBART class, testing (on transformers 2.10.0)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7zoZ4CFqBwm"
      },
      "source": [
        "# **Bart Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA-wSrQ8qm6g"
      },
      "source": [
        "!pip install -U transformers==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy8JWlpppIsX"
      },
      "source": [
        "\"\"\" BART configuration \"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "BART_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    \"facebook/bart-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/config.json\",\n",
        "    \"facebook/bart-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/config.json\",\n",
        "    \"facebook/bart-large-cnn\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/config.json\",\n",
        "    \"facebook/bart-large-xsum\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-xsum/config.json\",\n",
        "    \"facebook/mbart-large-en-ro\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json\",\n",
        "}\n",
        "\n",
        "\n",
        "class BartConfig(PretrainedConfig):\n",
        "    r\"\"\"\n",
        "        Configuration class for Bart. Parameters are renamed from the fairseq implementation\n",
        "    \"\"\"\n",
        "    model_type = \"bart\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        activation_dropout=0.0,\n",
        "        activation_function=\"gelu\",\n",
        "        vocab_size=50265,\n",
        "        d_model=1024,\n",
        "        encoder_ffn_dim=4096,\n",
        "        encoder_layers=12,\n",
        "        encoder_attention_heads=16,\n",
        "        decoder_ffn_dim=4096,\n",
        "        decoder_layers=12,\n",
        "        decoder_attention_heads=16,\n",
        "        encoder_layerdrop=0.0,\n",
        "        decoder_layerdrop=0.0,\n",
        "        attention_dropout=0.0,\n",
        "        dropout=0.1,\n",
        "        max_position_embeddings=1024,\n",
        "        encoder_max_position_embeddings=None,\t\n",
        "        decoder_max_position_embeddings=None,\n",
        "        init_std=0.02,\n",
        "        classifier_dropout=0.0,\n",
        "        num_labels=3,\n",
        "        is_encoder_decoder=True,\n",
        "        pad_token_id=1,\n",
        "        bos_token_id=0,\n",
        "        eos_token_id=2,\n",
        "        normalize_before=False,\n",
        "        add_final_layer_norm=False,\n",
        "        scale_embedding=False,\n",
        "        normalize_embedding=True,\n",
        "        static_position_embeddings=False,\n",
        "        add_bias_logits=False,\n",
        "        gradient_checkpointing=False,\n",
        "        **common_kwargs\n",
        "    ):\n",
        "        r\"\"\"\n",
        "            :class:`~transformers.BartConfig` is the configuration class for `BartModel`.\n",
        "            Examples:\n",
        "                config = BartConfig.from_pretrained('bart-large')\n",
        "                model = BartModel(config)\n",
        "        \"\"\"\n",
        "        if \"hidden_size\" in common_kwargs:\n",
        "            raise ValueError(\"hidden size is called d_model\")\n",
        "        super().__init__(\n",
        "            num_labels=num_labels,\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            is_encoder_decoder=is_encoder_decoder,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model  # encoder_embed_dim and decoder_embed_dim\n",
        "        self.encoder_ffn_dim = encoder_ffn_dim\n",
        "        self.encoder_layers = self.num_hidden_layers = encoder_layers\n",
        "        self.encoder_attention_heads = encoder_attention_heads\n",
        "        self.encoder_layerdrop = encoder_layerdrop\n",
        "        self.decoder_layerdrop = decoder_layerdrop\n",
        "        self.decoder_ffn_dim = decoder_ffn_dim\n",
        "        self.decoder_layers = decoder_layers\n",
        "        self.decoder_attention_heads = decoder_attention_heads\n",
        "        self.init_std = init_std  # Normal(0, this parameter)\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.encoder_max_position_embeddings = encoder_max_position_embeddings if encoder_max_position_embeddings else max_position_embeddings\t\n",
        "        self.decoder_max_position_embeddings = decoder_max_position_embeddings if decoder_max_position_embeddings else max_position_embeddings\n",
        "\n",
        "        # Params introduced for Mbart\n",
        "        self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n",
        "        self.normalize_embedding = normalize_embedding  # True for mbart, False otherwise\n",
        "        self.normalize_before = normalize_before  # combo of fairseq's encoder_ and decoder_normalize_before\n",
        "        self.add_final_layer_norm = add_final_layer_norm\n",
        "\n",
        "        # Params introduced for Marian\n",
        "        self.add_bias_logits = add_bias_logits\n",
        "        self.static_position_embeddings = static_position_embeddings\n",
        "\n",
        "        # 3 Types of Dropout\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Classifier stuff\n",
        "        self.classif_dropout = classifier_dropout\n",
        "        \n",
        "        # gradient_checkpointing\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        self.output_attentions = True\n",
        "\n",
        "    @property\n",
        "    def num_attention_heads(self) -> int:\n",
        "        return self.encoder_attention_heads\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.d_model\n",
        "\n",
        "    def is_valid_mbart(self) -> bool:\n",
        "        \"\"\"Is the configuration aligned with the MBART paper.\"\"\"\n",
        "        if self.normalize_before and self.add_final_layer_norm and self.scale_embedding:\n",
        "            return True\n",
        "        if self.normalize_before or self.add_final_layer_norm or self.scale_embedding:\n",
        "            logger.info(\"This configuration is a mixture of MBART and BART settings\")\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R67ClV7pg20"
      },
      "source": [
        "# **BartForConditionalGeneration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeTo497To9IC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "3e51a0c6-14a0-485d-eff4-e85adf6b577c"
      },
      "source": [
        "\"\"\"PyTorch BART model, ported from the fairseq repo.\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_callable\n",
        "from transformers.modeling_utils import PreTrainedModel, create_position_ids_from_input_ids\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/bart-large\",\n",
        "    \"facebook/bart-large-mnli\",\n",
        "    \"facebook/bart-large-cnn\",\n",
        "    \"facebook/bart-large-xsum\",\n",
        "    \"facebook/mbart-large-en-ro\",\n",
        "    # See all BART models at https://huggingface.co/models?filter=bart\n",
        "]\n",
        "\n",
        "\n",
        "BART_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class. Use it as a regular PyTorch Module and\n",
        "    refer to the PyTorch documentation for all matters related to general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
        "            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n",
        "\n",
        "\"\"\"\n",
        "BART_GENERATION_EXAMPLE = r\"\"\"\n",
        "    Examples::\n",
        "\n",
        "        from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "        # see ``examples/summarization/bart/evaluate_cnn.py`` for a longer example\n",
        "        model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
        "        tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
        "        ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "        inputs = tokenizer.batch_encode_plus([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
        "        # Generate Summary\n",
        "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
        "        print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "               Indices of input sequence tokens in the vocabulary. Use BartTokenizer.encode to produce them.\n",
        "            Padding will be ignored by default should you provide it.\n",
        "            Indices can be obtained using :class:`transformers.BartTokenizer.encode(text)`.\n",
        "        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Mask to avoid performing attention on padding token indices in input_ids.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`, defaults to :obj:`None`):\n",
        "            Tuple consists of (`last_hidden_state`, `optional`: `hidden_states`, `optional`: `attentions`)\n",
        "            `last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`, defaults to :obj:`None`) is a sequence of hidden-states at the output of the last layer of the encoder.\n",
        "            Used in the cross-attention of the decoder.\n",
        "        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Provide for translation and summarization training. By default, the model will create this tensor by shifting the input_ids right, following the paper.\n",
        "        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`, defaults to :obj:`None`):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in decoder_input_ids. Causal mask will also be used by default.\n",
        "            If you want to change padding behavior, you should read :func:`~transformers.modeling_bart._prepare_decoder_inputs` and modify.\n",
        "            See diagram 1 in the paper for more info on the default strategy\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def invert_mask(attention_mask):\n",
        "    assert attention_mask.dim() == 2\n",
        "    return attention_mask.eq(0)\n",
        "\n",
        "\n",
        "def _prepare_bart_decoder_inputs(\n",
        "    config, input_ids, decoder_input_ids=None, decoder_padding_mask=None, causal_mask_dtype=torch.float32\n",
        "):\n",
        "    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n",
        "    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n",
        "    Note: this is not called during generation\n",
        "    \"\"\"\n",
        "    pad_token_id = config.pad_token_id\n",
        "    if decoder_input_ids is None:\n",
        "        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n",
        "    bsz, tgt_len = decoder_input_ids.size()\n",
        "    if decoder_padding_mask is None:\n",
        "        decoder_padding_mask = make_padding_mask(decoder_input_ids, pad_token_id)\n",
        "    else:\n",
        "        decoder_padding_mask = invert_mask(decoder_padding_mask)\n",
        "    causal_mask = torch.triu(fill_with_neg_inf(torch.zeros(tgt_len, tgt_len)), 1).to(\n",
        "        dtype=causal_mask_dtype, device=decoder_input_ids.device\n",
        "    )\n",
        "    return decoder_input_ids, decoder_padding_mask, causal_mask\n",
        "\n",
        "\n",
        "class PretrainedBartModel(PreTrainedModel):\n",
        "    config_class = BartConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs\n",
        "\n",
        "\n",
        "def _make_linear_from_emb(emb):\n",
        "    vocab_size, emb_size = emb.weight.shape\n",
        "    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "    lin_layer.weight.data = emb.weight.data\n",
        "    return lin_layer\n",
        "\n",
        "\n",
        "# Helper Functions, mostly for making masks\n",
        "def _check_shapes(shape_1, shape2):\n",
        "    if shape_1 != shape2:\n",
        "        raise AssertionError(\"shape mismatch: {} != {}\".format(shape_1, shape2))\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
        "    prev_output_tokens = input_ids.clone()\n",
        "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "    return prev_output_tokens\n",
        "\n",
        "\n",
        "def make_padding_mask(input_ids, padding_idx=1):\n",
        "    \"\"\"True for pad tokens\"\"\"\n",
        "    padding_mask = input_ids.eq(padding_idx)\n",
        "    if not padding_mask.any():\n",
        "        padding_mask = None\n",
        "    return padding_mask\n",
        "\n",
        "\n",
        "# Helper Modules\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.self_attn = SelfAttention(\n",
        "            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.normalize_before = config.normalize_before\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
        "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
        "            for t_tgt, t_src is excluded (or masked out), =0 means it is\n",
        "            included in attention\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        x, attn_weights = self.self_attn(\n",
        "            query=x, key=x, key_padding_mask=encoder_padding_mask, need_weights=self.output_attentions\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return (x, attn_weights)\n",
        "\n",
        "\n",
        "class BartEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n",
        "    is a :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        embed_dim = embed_tokens.embedding_dim\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.encoder_max_position_embeddings\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.encoder_max_position_embeddings, embed_dim, self.padding_idx\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.encoder_max_position_embeddings, embed_dim, self.padding_idx,\n",
        "            )\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = LayerNorm(embed_dim) if config.normalize_embedding else nn.Identity()\n",
        "        # mbart has one extra layer_norm\n",
        "        self.layer_norm = LayerNorm(config.d_model) if config.normalize_before else None\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (LongTensor): tokens in the source language of shape\n",
        "                `(batch, src_len)`\n",
        "            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n",
        "        Returns:\n",
        "            Tuple comprised of:\n",
        "                - **x** (Tensor): the last encoder layer's output of\n",
        "                  shape `(src_len, batch, embed_dim)`\n",
        "                - **encoder_states** (List[Tensor]): all intermediate\n",
        "                  hidden states of shape `(src_len, batch, embed_dim)`.\n",
        "                  Only populated if *self.output_hidden_states:* is True.\n",
        "                - **all_attentions** (List[Tensor]): Attention weights for each layer.\n",
        "                During training might not be of length n_layers because of layer dropout.\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = invert_mask(attention_mask)\n",
        "\n",
        "        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        embed_pos = self.embed_positions(input_ids)\n",
        "        x = inputs_embeds + embed_pos\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        encoder_states, all_attentions = [], []\n",
        "        for encoder_layer in self.layers:\n",
        "            if self.output_hidden_states:\n",
        "                encoder_states.append(x)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                attn = None\n",
        "            else:\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False):\n",
        "                    x, attn = torch.utils.checkpoint.checkpoint(\n",
        "                        encoder_layer,\n",
        "                        x,\n",
        "                        attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    x, attn = encoder_layer(x, attention_mask)\n",
        "\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions.append(attn)\n",
        "\n",
        "        if self.layer_norm:\n",
        "            x = self.layer_norm(x)\n",
        "        if self.output_hidden_states:\n",
        "            encoder_states.append(x)\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        encoder_states = [hidden_state.transpose(0, 1) for hidden_state in encoder_states]\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, encoder_states, all_attentions\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = SelfAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.normalize_before = config.normalize_before\n",
        "\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = SelfAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attn_mask=None,\n",
        "        layer_state=None,\n",
        "        causal_mask=None,\n",
        "        decoder_padding_mask=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        residual = x\n",
        "\n",
        "        if layer_state is None:\n",
        "            layer_state = {}\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        # Self Attention\n",
        "\n",
        "        x, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            layer_state=layer_state,  # adds keys to layer state\n",
        "            key_padding_mask=decoder_padding_mask,\n",
        "            attn_mask=causal_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        # Cross attention\n",
        "        residual = x\n",
        "        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n",
        "        if self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "        x, _ = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_hidden_states,\n",
        "            key_padding_mask=encoder_attn_mask,\n",
        "            layer_state=layer_state,  # mutates layer state\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            layer_state,\n",
        "        )  # just self_attn weights for now, following t5, layer_state = cache for decoding\n",
        "\n",
        "\n",
        "class BartDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n",
        "    is a :class:`DecoderLayer`.\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, config.pad_token_id\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, self.padding_idx,\n",
        "            )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderLayer(config) for _ in range(config.decoder_layers)]\n",
        "        )  # type: List[DecoderLayer]\n",
        "        self.layernorm_embedding = LayerNorm(config.d_model) if config.normalize_embedding else nn.Identity()\n",
        "        self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_padding_mask,\n",
        "        decoder_padding_mask,\n",
        "        decoder_causal_mask,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        **unused,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Includes several features from \"Jointly Learning to Align and\n",
        "        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n",
        "\n",
        "        Args:\n",
        "            input_ids (LongTensor): previous decoder outputs of shape\n",
        "                `(batch, tgt_len)`, for teacher forcing\n",
        "            encoder_hidden_states: output from the encoder, used for\n",
        "                encoder-side attention\n",
        "            encoder_padding_mask: for ignoring pad tokens\n",
        "            decoder_cached_states (dict or None): dictionary used for storing state during generation\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
        "                - hidden states\n",
        "                - attentions\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if encoder_padding_mask is not None:\n",
        "            encoder_padding_mask = invert_mask(encoder_padding_mask)\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_ids, use_cache=use_cache)\n",
        "\n",
        "        if use_cache:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "            positions = positions[:, -1:]  # happens after we embed them\n",
        "            # assert input_ids.ne(self.padding_idx).any()\n",
        "\n",
        "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        x += positions\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = ()\n",
        "        all_self_attns = ()\n",
        "        next_decoder_cache = []\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states += (x,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            layer_state = decoder_cached_states[idx] if decoder_cached_states is not None else None\n",
        "\n",
        "            x, layer_self_attn, layer_past = decoder_layer(\n",
        "                x,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attn_mask=encoder_padding_mask,\n",
        "                decoder_padding_mask=decoder_padding_mask,\n",
        "                layer_state=layer_state,\n",
        "                causal_mask=decoder_causal_mask,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache.append(layer_past.copy())\n",
        "\n",
        "            if self.layer_norm and (idx == len(self.layers) - 1):  # last layer of mbart\n",
        "                x = self.layer_norm(x)\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_self_attn,)\n",
        "\n",
        "        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        all_hidden_states = [hidden_state.transpose(0, 1) for hidden_state in all_hidden_states]\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        if use_cache:\n",
        "            next_cache = ((encoder_hidden_states, encoder_padding_mask), next_decoder_cache)\n",
        "        else:\n",
        "            next_cache = None\n",
        "        return x, next_cache, all_hidden_states, list(all_self_attns)\n",
        "\n",
        "\n",
        "def _reorder_buffer(attn_cache, new_order):\n",
        "    for k, input_buffer_k in attn_cache.items():\n",
        "        if input_buffer_k is not None:\n",
        "            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n",
        "    return attn_cache\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n",
        "\n",
        "    def _shape(self, tensor, dim_0, bsz):\n",
        "        return tensor.contiguous().view(dim_0, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query,\n",
        "        key: Optional[Tensor],\n",
        "        key_padding_mask: Optional[Tensor] = None,\n",
        "        layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "        need_weights=False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n",
        "        static_kv: bool = self.encoder_decoder_attention\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        # get here for encoder decoder cause of static_kv\n",
        "        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n",
        "            saved_state = layer_state.get(self.cache_key, {})\n",
        "            if \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute key and value if they are static\n",
        "                if static_kv:\n",
        "                    key = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "            layer_state = {}\n",
        "\n",
        "        q = self.q_proj(query) * self.scaling\n",
        "        if static_kv:\n",
        "            if key is None:\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self._shape(q, tgt_len, bsz)\n",
        "        if k is not None:\n",
        "            k = self._shape(k, -1, bsz)\n",
        "        if v is not None:\n",
        "            v = self._shape(v, -1, bsz)\n",
        "\n",
        "        if saved_state is not None:\n",
        "            k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)\n",
        "\n",
        "        # Update cache\n",
        "        layer_state[self.cache_key] = {\n",
        "            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_key_padding_mask\": key_padding_mask if not static_kv else None,\n",
        "        }\n",
        "\n",
        "        assert k is not None\n",
        "        src_len = k.size(1)\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attn_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "        assert key_padding_mask is None or key_padding_mask.size()[:2] == (bsz, src_len,)\n",
        "\n",
        "        if key_padding_mask is not None:  # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training,)\n",
        "\n",
        "        assert v is not None\n",
        "        attn_output = torch.bmm(attn_probs, v)\n",
        "        assert attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights = None\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv, bsz):\n",
        "        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "        if \"prev_key\" in saved_state:\n",
        "            _prev_key = saved_state[\"prev_key\"]\n",
        "            assert _prev_key is not None\n",
        "            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                k = prev_key\n",
        "            else:\n",
        "                assert k is not None\n",
        "                k = torch.cat([prev_key, k], dim=1)\n",
        "        if \"prev_value\" in saved_state:\n",
        "            _prev_value = saved_state[\"prev_value\"]\n",
        "            assert _prev_value is not None\n",
        "            prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                v = prev_value\n",
        "            else:\n",
        "                assert v is not None\n",
        "                v = torch.cat([prev_value, v], dim=1)\n",
        "        assert k is not None and v is not None\n",
        "        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\"prev_key_padding_mask\", None)\n",
        "        key_padding_mask = self._cat_prev_key_padding_mask(\n",
        "            key_padding_mask, prev_key_padding_mask, bsz, k.size(1), static_kv\n",
        "        )\n",
        "        return k, v, key_padding_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_prev_key_padding_mask(\n",
        "        key_padding_mask: Optional[Tensor],\n",
        "        prev_key_padding_mask: Optional[Tensor],\n",
        "        batch_size: int,\n",
        "        src_len: int,\n",
        "        static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None:\n",
        "            if static_kv:\n",
        "                new_key_padding_mask = prev_key_padding_mask\n",
        "            else:\n",
        "                new_key_padding_mask = torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)\n",
        "\n",
        "        elif key_padding_mask is not None:\n",
        "            filler = torch.zeros(\n",
        "                batch_size,\n",
        "                src_len - key_padding_mask.size(1),\n",
        "                dtype=key_padding_mask.dtype,\n",
        "                device=key_padding_mask.device,\n",
        "            )\n",
        "            new_key_padding_mask = torch.cat([filler, key_padding_mask], dim=1)\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "\n",
        "class BartClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    # This can trivially be shared with RobertaClassificationHead\n",
        "\n",
        "    def __init__(\n",
        "        self, input_dim, inner_dim, num_classes, pooler_dropout,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inner_dim)\n",
        "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    Padding ids are ignored by either offsetting based on padding_idx\n",
        "    or by setting padding_idx to None and ensuring that the appropriate\n",
        "    position ids are passed to the forward function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_embeddings: int, embedding_dim: int, padding_idx: int,\n",
        "    ):\n",
        "        # if padding_idx is specified then offset the embedding ids by\n",
        "        # this index and adjust num_embeddings appropriately\n",
        "        assert padding_idx is not None\n",
        "        num_embeddings += padding_idx + 1  # WHY?\n",
        "        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "    def forward(self, input, use_cache=False):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        if use_cache:  # the position is our current step in the decoded sequence\n",
        "            pos = int(self.padding_idx + input.size(1))\n",
        "            positions = input.data.new(1, 1).fill_(pos)\n",
        "        else:\n",
        "            positions = create_position_ids_from_input_ids(input, self.padding_idx)\n",
        "        return super().forward(positions)\n",
        "\n",
        "\n",
        "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            from apex.normalization import FusedLayerNorm\n",
        "\n",
        "            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n",
        "        except ImportError:\n",
        "            pass\n",
        "    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n",
        "\n",
        "\n",
        "def fill_with_neg_inf(t):\n",
        "    \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n",
        "    return t.float().fill_(float(\"-inf\")).type_as(t)\n",
        "\n",
        "\n",
        "def _filter_out_falsey_values(tup) -> Tuple:\n",
        "    \"\"\"Remove entries that are None or [] from an iterable.\"\"\"\n",
        "    return tuple(x for x in tup if isinstance(x, torch.Tensor) or x)\n",
        "\n",
        "\n",
        "# Public API\n",
        "def _get_shape(t):\n",
        "    return getattr(t, \"shape\", None)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare BART Model outputting raw hidden-states without any specific head on top.\", BART_START_DOCSTRING,\n",
        ")\n",
        "class BartModel(PretrainedBartModel):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.encoder = BartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        encoder_outputs: Optional[Tuple] = None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "    ):\n",
        "\n",
        "        # make masks if user doesn't supply\n",
        "        if not use_cache:\n",
        "            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n",
        "                self.config,\n",
        "                input_ids,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                decoder_padding_mask=decoder_attention_mask,\n",
        "                causal_mask_dtype=self.shared.weight.dtype,\n",
        "            )\n",
        "        else:\n",
        "            decoder_padding_mask, causal_mask = None, None\n",
        "\n",
        "        assert decoder_input_ids is not None\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        assert isinstance(encoder_outputs, tuple)\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            decoder_input_ids,\n",
        "            encoder_outputs[0],\n",
        "            attention_mask,\n",
        "            decoder_padding_mask,\n",
        "            decoder_causal_mask=causal_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        # Attention and hidden_states will be [] or None if they aren't needed\n",
        "        decoder_outputs: Tuple = _filter_out_falsey_values(decoder_outputs)\n",
        "        assert isinstance(decoder_outputs[0], torch.Tensor)\n",
        "        encoder_outputs: Tuple = _filter_out_falsey_values(encoder_outputs)\n",
        "        return decoder_outputs + encoder_outputs\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.shared)  # make it on the fly\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The BART Model with a language modeling head. Can be used for summarization.\",\n",
        "    BART_START_DOCSTRING + BART_GENERATION_EXAMPLE,\n",
        ")\n",
        "class BartForConditionalGeneration(PretrainedBartModel):\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        base_model = BartModel(config)\n",
        "        self.model = base_model\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        old_num_tokens = self.model.shared.num_embeddings\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self.model.shared = new_embeddings\n",
        "        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        lm_labels=None,\n",
        "        use_cache=False,\n",
        "        **unused\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        lm_labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n",
        "            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n",
        "            with labels\n",
        "            in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:\n",
        "        masked_lm_loss (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Masked language modeling loss.\n",
        "        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`)\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "            # Mask filling only works for bart-large\n",
        "            from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "            tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
        "            TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
        "            model = BartForConditionalGeneration.from_pretrained('bart-large')\n",
        "            input_ids = tokenizer.batch_encode_plus([TXT], return_tensors='pt')['input_ids']\n",
        "            logits = model(input_ids)[0]\n",
        "            masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "            probs = logits[0, masked_index].softmax(dim=0)\n",
        "            values, predictions = probs.topk(5)\n",
        "            tokenizer.decode(predictions).split()\n",
        "            # ['good', 'great', 'all', 'really', 'very']\n",
        "        \"\"\"\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)\n",
        "        outputs = (lm_logits,) + outputs[1:]  # Add cache, hidden states and attention if they are here\n",
        "        if lm_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # TODO(SS): do we need to ignore pad tokens in lm_labels?\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), lm_labels.view(-1))\n",
        "            outputs = (masked_lm_loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def prepare_inputs_for_generation(self, decoder_input_ids, past, attention_mask, use_cache, **kwargs):\n",
        "        assert past is not None, \"past has to be defined for encoder_outputs\"\n",
        "\n",
        "        # first step, decoder_cached_states are empty\n",
        "        if not past[1]:\n",
        "            encoder_outputs, decoder_cached_states = past, None\n",
        "        else:\n",
        "            encoder_outputs, decoder_cached_states = past\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"decoder_cached_states\": decoder_cached_states,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_logits_for_generation(self, logits, cur_len, max_length):\n",
        "        if cur_len == 1:\n",
        "            self._force_token_ids_generation(logits, self.config.bos_token_id)\n",
        "        if cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
        "            self._force_token_ids_generation(logits, self.config.eos_token_id)\n",
        "        return logits\n",
        "\n",
        "    def _force_token_ids_generation(self, scores, token_ids) -> None:\n",
        "        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\n",
        "        if isinstance(token_ids, int):\n",
        "            token_ids = [token_ids]\n",
        "        all_but_token_ids_mask = torch.tensor(\n",
        "            [x for x in range(self.config.vocab_size) if x not in token_ids],\n",
        "            dtype=torch.long,\n",
        "            device=next(self.parameters()).device,\n",
        "        )\n",
        "        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n",
        "        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        ((enc_out, enc_mask), decoder_cached_states) = past\n",
        "        reordered_past = []\n",
        "        for layer_past in decoder_cached_states:\n",
        "            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n",
        "            layer_past_new = {\n",
        "                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n",
        "            }\n",
        "            reordered_past.append(layer_past_new)\n",
        "\n",
        "        new_enc_out = enc_out if enc_out is None else enc_out.index_select(0, beam_idx)\n",
        "        new_enc_mask = enc_mask if enc_mask is None else enc_mask.index_select(0, beam_idx)\n",
        "\n",
        "        past = ((new_enc_out, new_enc_mask), reordered_past)\n",
        "        return past\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.encoder\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks. \"\"\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartForSequenceClassification(PretrainedBartModel):\n",
        "    def __init__(self, config: BartConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.model = BartModel(config)\n",
        "        self.classification_head = BartClassificationHead(\n",
        "            config.d_model, config.d_model, config.num_labels, config.classif_dropout,\n",
        "        )\n",
        "        self.model._init_weights(self.classification_head.dense)\n",
        "        self.model._init_weights(self.classification_head.out_proj)\n",
        "\n",
        "    @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BartConfig`) and inputs:\n",
        "            loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
        "                Classification loss (cross entropy)\n",
        "            logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
        "                Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "                Attentions weights after the attention softmax, used to compute the weighted average in the\n",
        "                self-attention\n",
        "                heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        from transformers import BartTokenizer, BartForSequenceClassification\n",
        "        import torch\n",
        "\n",
        "        tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
        "        model = BartForSequenceClassification.from_pretrained('bart-large')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\",\n",
        "        add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        \"\"\"\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "        )\n",
        "        x = outputs[0]  # last hidden state\n",
        "        eos_mask = input_ids.eq(self.config.eos_token_id)\n",
        "        if len(torch.unique(eos_mask.sum(1))) > 1:\n",
        "            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n",
        "        sentence_representation = x[eos_mask, :].view(x.size(0), -1, x.size(-1))[:, -1, :]\n",
        "        logits = self.classification_head(sentence_representation)\n",
        "        # Prepend logits\n",
        "        outputs = (logits,) + outputs[1:]  # Add hidden states and attention if they are here\n",
        "        if labels is not None:  # prepend loss to output,\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        if embedding_dim % 2 != 0:\n",
        "            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        \"\"\"Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.\n",
        "            The cos features are in the 2nd half of the vector. [dim // 2:]\n",
        "        \"\"\"\n",
        "        n_pos, dim = out.shape\n",
        "        position_enc = np.array(\n",
        "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
        "        )\n",
        "        out[:, 0 : dim // 2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))  # This line breaks for odd n_pos\n",
        "        out[:, dim // 2 :] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        out.detach_()\n",
        "        out.requires_grad = False\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids, use_cache=False):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        if use_cache:\n",
        "            positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n",
        "        else:\n",
        "            # starts at 0, ends at 1-seq_len\n",
        "            positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
        "        return super().forward(positions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4fc81c246344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPretrainedBartModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mbase_model_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-4fc81c246344>\u001b[0m in \u001b[0;36mPretrainedBartModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPretrainedBartModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mbase_model_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BartConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-fauWiVptTV"
      },
      "source": [
        "# **LongBartForConditionalGeneration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JexdbZbogOR"
      },
      "source": [
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from transformers.modeling_longformer import LongformerSelfAttention\n",
        "\n",
        "\n",
        "class LongBartForConditionalGeneration(BartForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.model.encoder.layers):\n",
        "            # replace the `modeling_bart.SelfAttention` object with `LongformerSelfAttention`\n",
        "            layer.self_attn = LongformerSelfAttentionForBart(config, layer_id=i)\n",
        "\n",
        "\n",
        "class LongformerSelfAttentionForBart(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.longformer_self_attn = LongformerSelfAttention(config, layer_id=layer_id)\n",
        "        self.output = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        query,\n",
        "        key: Optional[Tensor],\n",
        "        key_padding_mask: Optional[Tensor] = None,\n",
        "        layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "        need_weights=False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        \n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "\n",
        "        # LongformerSelfAttention expects this shape\n",
        "        query = query.view(bsz, tgt_len, embed_dim)\n",
        "\n",
        "        outputs = self.longformer_self_attn(\n",
        "            query,\n",
        "            attention_mask=attn_mask,\n",
        "            head_mask=None,\n",
        "            encoder_hidden_states=None,\n",
        "            encoder_attention_mask=None,\n",
        "        )\n",
        "\n",
        "        attn_output = outputs[0] \n",
        "        attn_output = attn_output.contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn_output = self.output(attn_output)\n",
        "\n",
        "        return (attn_output,) + outputs[1:] if len(outputs) == 2 else (attn_output, None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn7I0nMPpyV6"
      },
      "source": [
        "# **Test model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYOQarg7rFx_"
      },
      "source": [
        "from transformers import BartTokenizer\n",
        "import torch\n",
        "model_path = \"/content/drive/MyDrive/longformers/models/long-bart-large-4096\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ZkbQ0wlCAn"
      },
      "source": [
        "model = LongBartForConditionalGeneration.from_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Hz3uaSq4qb"
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnuoI3LallLC",
        "outputId": "253b7987-32fb-4eeb-8137-6d98f17e6542"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "padding = \"max_length\" \n",
        "\n",
        "text=\"\"\"On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View's securities as well as the securities of another microcap issuer, West Coast Ventures Group Corp.  The SEC further announced the institution of an order suspending trading in the securities of HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.In a complaint filed in the U.S. District Court for the Eastern District of New York, the SEC alleges that Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for HD View's common stock.  The scheme involved an undercover FBI agent who described his business as manipulating U.S. stocks through pump-and-dump schemes.  Kyriacou and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit.The SEC's complaint against Beaufort and Kyriacou alleges that they:opened brokerage accounts for the undercover agent in the names of nominees in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the undercover agent could create the false appearance that HD View's stock was liquid in advance of a pump-and-dump by \"gam[ing] the market\" through matched tradesexecuted multiple purchase orders of HD View shares with the understanding that Beaufort's client had arranged for an associate to simultaneously offer an equivalent number of shares at the same priceA second complaint filed by the SEC in the U.S. District Court for the Eastern District of New York alleges that in a series of recorded telephone conversations with the undercover agent, HD View CEO Dennis Mancino and William T. Hirschy agreed to manipulate HD View's common stock by using the agent's network of brokers to generate fraudulent retail demand for the stock in exchange for a kickback from the trading proceeds.  According to the complaint, the three men agreed that Mancino and Hirschy would manipulate HD View stock to a higher price before using the agent's brokers to liquidate their positions at an artificially inflated price.  The SEC's complaint also alleges that Mancino and Hirschy executed a \"test trade\" on Jan. 31, 2018, coordinated by the agent, consisting of a sell order placed by the defendants filled by an opposing purchase order placed by a broker into an account at Beaufort.  Unbeknownst to Mancino and Hirschy, the Beaufort account used for this trade was a nominal account that was opened and funded by the agent.  The SEC's complaint also alleges that, prior to their contact with the undercover agent, Mancino and Hirschy manipulated the market for HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with â€“including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC â€“ to effect manipulative \"matched trades.\"The SEC's complaint against Beaufort and Kyriacou charges the defendants with violating Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder.  The SEC also charged Hirschy, Mancino, and their corporate entities with violating Section 17(a)(1) of the Securities Act of 1933, Sections 9(a)(1), 9(a)(2), and 10(b) of the Exchange Act and Rules 10b-5(a) and (c) thereunder.  The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, and penny stock bars from Beaufort and Kyriacou.  With respect to Hirschy, Mancino, and their corporate entities, the SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, penny stock bars, and an officer-and-director bar against Mancino.The investigation was conducted in the SEC's New York Regional Office by Tejal Shah and Joseph Darragh, Lorraine Collazo, and Michael D. Paley of the Microcap Fraud Task Force and supervised by Lara S. Mehraban, and in Washington, D.C. by Patrick L. Feeney, Robert Nesbitt, and Kevin Guerrero, and supervised by Antonia Chion.  Preethi Krishnamurthy and Ms. Shah will lead the SEC's litigation against Beaufort and Kyriacou.  Ann H. Petalas and Mr. Feeney, under the supervision of Cheryl Crumpton, will handle the SEC's litigation against Mancino, Hirschy, and their entities.  The SEC appreciates the assistance of the Office of the United States Attorney for the Eastern District of New York, the Federal Bureau of Investigation, the Internal Revenue Service, the Alberta Securities Commission, the Ontario Securities Commission, the Financial Conduct Authority of the United Kingdom, and the Financial Industry Regulatory Authority.The Commission's investigation in this matter is continuing.\"\"\"\n",
        "input_tokenized = tokenizer.encode(text, return_tensors='pt', max_length=4096).to(device)\n",
        "\n",
        "dim = list(input_tokenized.size())\n",
        "print(\"Input Shape (before padding to max_length): {}\".format(dim))\n",
        "\n",
        "\n",
        "input_tokenized = tokenizer.encode(text, return_tensors='pt',padding=padding,pad_to_max_length=True, padding_side='right', max_length=4096,truncation=True).to(device)\n",
        "\n",
        "dim = list(input_tokenized.size())\n",
        "print(\"Input Shape: {} (padded to max_length)\".format(dim))\n",
        "\n",
        "summary_ids = model.generate(input_tokenized,\n",
        "                                  num_beams=4,\n",
        "                                  no_repeat_ngram_size=3,\n",
        "                                  length_penalty=1.2,\n",
        "                                  min_length=350,\n",
        "                                  max_length=500)\n",
        "\n",
        "dim = list(summary_ids.size())\n",
        "print(\"Summary Shape: \", dim)\n",
        "  \n",
        "summ = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
        "\n",
        "print(\"Text: \")\n",
        "print(text)\n",
        "print(\"Summary: \")\n",
        "print(summ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shape (before padding to max_length): [1, 1124]\n",
            "Input Shape: [1, 4096] (padded to max_length)\n",
            "Summary Shape:  [1, 499]\n",
            "Text: \n",
            "On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating HD View's securities as well as the securities of another microcap issuer, West Coast Ventures Group Corp.  The SEC further announced the institution of an order suspending trading in the securities of HD View.These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York.In a complaint filed in the U.S. District Court for the Eastern District of New York, the SEC alleges that Beaufort Securities Ltd. and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for HD View's common stock.  The scheme involved an undercover FBI agent who described his business as manipulating U.S. stocks through pump-and-dump schemes.  Kyriacou and the agent discussed depositing large blocks of microcap stock in Beaufort accounts, driving up the price of the stock through promotions, manipulating the stock's price and volume through matched trades, and then selling the shares for a large profit.The SEC's complaint against Beaufort and Kyriacou alleges that they:opened brokerage accounts for the undercover agent in the names of nominees in order to conceal his identity and his connection to the anticipated trading activity in the accountssuggested that the undercover agent could create the false appearance that HD View's stock was liquid in advance of a pump-and-dump by \"gam[ing] the market\" through matched tradesexecuted multiple purchase orders of HD View shares with the understanding that Beaufort's client had arranged for an associate to simultaneously offer an equivalent number of shares at the same priceA second complaint filed by the SEC in the U.S. District Court for the Eastern District of New York alleges that in a series of recorded telephone conversations with the undercover agent, HD View CEO Dennis Mancino and William T. Hirschy agreed to manipulate HD View's common stock by using the agent's network of brokers to generate fraudulent retail demand for the stock in exchange for a kickback from the trading proceeds.  According to the complaint, the three men agreed that Mancino and Hirschy would manipulate HD View stock to a higher price before using the agent's brokers to liquidate their positions at an artificially inflated price.  The SEC's complaint also alleges that Mancino and Hirschy executed a \"test trade\" on Jan. 31, 2018, coordinated by the agent, consisting of a sell order placed by the defendants filled by an opposing purchase order placed by a broker into an account at Beaufort.  Unbeknownst to Mancino and Hirschy, the Beaufort account used for this trade was a nominal account that was opened and funded by the agent.  The SEC's complaint also alleges that, prior to their contact with the undercover agent, Mancino and Hirschy manipulated the market for HD View and for West Coast by using brokerage accounts that they owned, controlled, or were associated with â€“including TJM Investments Inc., DJK Investments 10 Inc., WT Consulting Group LLC â€“ to effect manipulative \"matched trades.\"The SEC's complaint against Beaufort and Kyriacou charges the defendants with violating Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder.  The SEC also charged Hirschy, Mancino, and their corporate entities with violating Section 17(a)(1) of the Securities Act of 1933, Sections 9(a)(1), 9(a)(2), and 10(b) of the Exchange Act and Rules 10b-5(a) and (c) thereunder.  The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, and penny stock bars from Beaufort and Kyriacou.  With respect to Hirschy, Mancino, and their corporate entities, the SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, penny stock bars, and an officer-and-director bar against Mancino.The investigation was conducted in the SEC's New York Regional Office by Tejal Shah and Joseph Darragh, Lorraine Collazo, and Michael D. Paley of the Microcap Fraud Task Force and supervised by Lara S. Mehraban, and in Washington, D.C. by Patrick L. Feeney, Robert Nesbitt, and Kevin Guerrero, and supervised by Antonia Chion.  Preethi Krishnamurthy and Ms. Shah will lead the SEC's litigation against Beaufort and Kyriacou.  Ann H. Petalas and Mr. Feeney, under the supervision of Cheryl Crumpton, will handle the SEC's litigation against Mancino, Hirschy, and their entities.  The SEC appreciates the assistance of the Office of the United States Attorney for the Eastern District of New York, the Federal Bureau of Investigation, the Internal Revenue Service, the Alberta Securities Commission, the Ontario Securities Commission, the Financial Conduct Authority of the United Kingdom, and the Financial Industry Regulatory Authority.The Commission's investigation in this matter is continuing.\n",
            "Summary: \n",
            "These charges arise in part from an undercover operation by the Federal Bureau of Investigation, which also resulted in related criminal prosecutions against these defendants by the Office of the United States Attorney for the Eastern District of New York. On March 2, 2018, the Securities and Exchange Commission announced securities fraud charges against a U.K.-based broker-dealer and its investment manager in connection with manipulative trading in the securities of HD View 360 Inc., a U.,S.-based microcap issuer.  The SEC also announced charges against HD View's CEO, another individual, and three entities they control for manipulating MHV's stock in the Securities Commission, and the Financial Industry Regulatory Authority's Fraud.The Commission's investigation into the manipulation of MHV was conducted in this matter is continuing. The SEC is seeking injunctions, disgorgement, prejudgment interest, penalties, the forfeiture of the stock bars from HD View, and their corporate entities, SEC's enforcement actions against MHV and their entities.In a complaint filed in the U.S. District Court for the District of Columbia, the SEC alleges that in the course of its investigation into MHV, the Financial Regulatory Authority, the Federal Deposit Insurance Corp., the Securities Exchange Commission, the United Kingdom, and Peter Kyriacou, an investment manager at Beaufort, manipulated the market for MHV. The scheme involved an undercover FBI agent who described his business as manipulating U. S. stocks through pump-and-dump schemes.  He said that under the supervision of Cheryl and the SEC, he would lead the investigation against Mancino, Hirschy, and New York, and that the SEC would assist in the investigation of the Federal States Attorney, Mr. Robert, and Mr. Gerard.  Mr. Kevan, and Ms. Gerard, and Lora, and Lorasas. Mr. Gerard, and Mrs. Loras, and Robert. Mrion, and Dr. Robert.  Robert, Dr. John, and H.H. Hirschey.  H.R.Hirschey, and M. Hirschen.  Khirsh.  Ker.  Kar.  Kirschen, and K. K. Kirsch.  K.K. Kirch.  Kerry.  Krish.  Har.  Th. Th. Th. Thir. thir.Thir. Thyr.  thir.thirth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjuNWr-CRwB-"
      },
      "source": [
        "# **Finetune long-bart (not working)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIzYjpFwqIf"
      },
      "source": [
        "**Using transformer folder for imports (running on 4.3.0) (2.10 for longbart)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITcHWLMsrtfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6237c110-1dce-421b-93a1-e36fc0e61793"
      },
      "source": [
        "!python /content/drive/MyDrive/Finetune/run-longbart.py  \\\n",
        "    --model_name_or_path /content/drive/MyDrive/longformers/models/long-bart-large-4096/ \\\n",
        "    --longmodel \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --task summarization \\\n",
        "    --max_source_length=1024 \\\n",
        "    --max_target_length=400 \\\n",
        "    --num_beams=3 \\\n",
        "    --min_summ_length=350 \\\n",
        "    --max_summ_length=450 \\\n",
        "    --length_penalty=1.2 \\\n",
        "    --train_file /content/drive/MyDrive/longformers/data/train.csv \\\n",
        "    --validation_file /content/drive/MyDrive/longformers/data/valid.csv \\\n",
        "    --output_dir /content/output/long-bart-large-4096 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --predict_with_generate \\\n",
        "    --text_column Text \\\n",
        "    --summary_column Summary "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformers version:  2.11.0\n",
            "2021-02-14 14:34:11.018869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/14/2021 14:34:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/14/2021 14:34:12 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/output/long-bart-large-4096', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Feb14_14-34-12_e1ca0a88b4af', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/output/long-bart-large-4096', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], sortish_sampler=False, predict_with_generate=True)\n",
            "Using custom data configuration default\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-dc5e0ef39cddc6b7/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n",
            "loading configuration file /content/drive/MyDrive/longformers/models/long-bart-large-4096/config.json\n",
            "Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_max_position_embeddings\": 1024,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"encoder_max_position_embeddings\": 4096,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/longformers/models/long-bart-large-4096/config.json\n",
            "Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_max_position_embeddings\": 1024,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"encoder_max_position_embeddings\": 4096,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Model name '/content/drive/MyDrive/longformers/models/long-bart-large-4096/' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming '/content/drive/MyDrive/longformers/models/long-bart-large-4096/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "Didn't find file /content/drive/MyDrive/longformers/models/long-bart-large-4096/tokenizer.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/longformers/models/long-bart-large-4096/added_tokens.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/longformers/models/long-bart-large-4096/vocab.json\n",
            "loading file /content/drive/MyDrive/longformers/models/long-bart-large-4096/merges.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/longformers/models/long-bart-large-4096/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/longformers/models/long-bart-large-4096/tokenizer_config.json\n",
            "02/14/2021 14:34:13 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/longformers/models/long-bart-large-4096/config.json\n",
            "02/14/2021 14:34:13 - INFO - transformers.configuration_utils -   Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_max_position_embeddings\": 1024,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"encoder_max_position_embeddings\": 4096,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/14/2021 14:34:13 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/longformers/models/long-bart-large-4096/pytorch_model.bin\n",
            "100% 1/1 [00:00<00:00, 25.33ba/s]\n",
            "100% 1/1 [00:00<00:00, 46.09ba/s]\n",
            "The following columns in the training set don't have a corresponding argument in `LongBartForConditionalGeneration.forward` and have been ignored: labels.\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongBartForConditionalGeneration.forward` and have been ignored: labels.\n",
            "***** Running training *****\n",
            "  Num examples = 2\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3\n",
            "  0% 0/3 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Finetune/run-longbart.py\", line 633, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/Finetune/run-longbart.py\", line 486, in main\n",
            "    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
            "  File \"/content/drive/MyDrive/Finetune/transformersv4/trainer.py\", line 881, in train\n",
            "    tr_loss += self.training_step(model, inputs)\n",
            "  File \"/content/drive/MyDrive/Finetune/transformersv4/trainer.py\", line 1245, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/content/drive/MyDrive/Finetune/transformersv4/trainer.py\", line 1275, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Finetune/bart.py\", line 965, in forward\n",
            "    use_cache=use_cache,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Finetune/bart.py\", line 856, in forward\n",
            "    use_cache=use_cache,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Finetune/bart.py\", line 529, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Finetune/bart.py\", line 400, in forward\n",
            "    layer_state=layer_state,  # mutates layer state\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Finetune/bart.py\", line 660, in forward\n",
            "    attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training,)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 983, in dropout\n",
            "    else _VF.dropout(input, p, training))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.76 GiB total capacity; 13.47 GiB already allocated; 15.75 MiB free; 13.71 GiB reserved in total by PyTorch)\n",
            "  0% 0/3 [00:01<?, ?it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXNJjOK4wby9"
      },
      "source": [
        "**Without transformer folder (running on 2.10.0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJfSzbDWwhZi"
      },
      "source": [
        "!python /content/drive/MyDrive/Finetune/longbart-finetune/custom_longbart_run.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/longformers/models/long-bart-large-4096 \\\n",
        "    --longmodel \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --task summarization \\\n",
        "    --max_source_length=1024 \\\n",
        "    --max_target_length=400 \\\n",
        "    --num_beams=3 \\\n",
        "    --min_summ_length=350 \\\n",
        "    --max_summ_length=450 \\\n",
        "    --length_penalty=1.2 \\\n",
        "    --train_file /content/drive/MyDrive/longformers/data/train.csv \\\n",
        "    --validation_file /content/drive/MyDrive/longformers/data/valid.csv \\\n",
        "    --output_dir /content/output/long-bart-large-4096 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --predict_with_generate \\\n",
        "    --text_column Text \\\n",
        "    --summary_column Summary "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
Converting bart to longbart:

Reusing the high-level encoder-decoder architecture of BART using BartForConditionalGeneration
Replacing BART encoder's attention layers with the LongformerSelfAttentionForBart (from transformers.modeling_longformer)
Increasing max_pos (positional embeddings) to 4096 (any number > 1024)
Increasing the attention_window to 1024
